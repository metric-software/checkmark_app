// This code is responsible for collecting data from the different providers,
// processing it and writing it to the different output files. .csv file
// contains the "per second" raw data from the run. Most values there are the
// avg/min/max or total of the metric over the second. The datapoints are
// defined in BenchmarkDataPoint.h .json file is generated by the game, and then
// just copied to the output folder. It contains few simple values, and its
// mainly going to be used for double checking the .csv file values.
// RustBenchmarkFinder.h is responsible for finding the file. the _specs.txt
// file contains metadata about the run, like the system specs, and the game
// settings used for the run. And some other details which are used for analysis
// later on... The functionality for this is in ConstantSystemInfo.h

// BenchmarkStateTracker is responsible for finding the correct start and
// endpoints of the run. PresentDataExports is the ETW interface for
// frametime/fps/cpu and gpu time data. PdhInterface handles system metrics
// collection, and DiskPerformanceTracker collects disk metrics

#include "benchmark/BenchmarkManager.h"
#include "benchmark/BenchmarkResultFileManager.h"
#include "../logging/Logger.h"
#include "../ApplicationSettings.h"
#include "../network/api/BenchmarkApiClient.h"
#include "../network/serialization/PublicExportBuilder.h"

#include <chrono>
#include <fstream>
#include <iomanip>
#include <iostream>
#include <mutex>
#include <set>
#include <sstream>
#include <string>
#include <thread>
#include <vector>

#include <QCoreApplication>
#include <QCryptographicHash>
#include <QDateTime>
#include <QDir>
#include <QFileInfo>
#include <QRandomGenerator>
#include <QTimer>
#include <windows.h>
#include <evntcons.h>
#include <evntrace.h>
#include <shellapi.h>
#include <strsafe.h>
#include <tdh.h>
#include <tlhelp32.h>
#include <wmistr.h>

#include "benchmark/BenchmarkConstants.h"
#include "benchmark/BenchmarkSpecsFileManager.h"
#include "benchmark/DemoFileManager.h"
#include "benchmark/RustBenchmarkFinder.h"

#include "hardware/PdhInterface.h"
#include "hardware/ConstantSystemInfo.h"
#include "optimization/ExportSettings.h"
#include "optimization/OptimizationEntity.h"
#include "profiles/UserSystemProfile.h"

namespace {
std::mutex g_sessionMutex;
std::set<std::wstring> g_activeSessions;
BenchmarkManager* instance = nullptr;
}  // namespace

inline void LogError(const std::string& msg) {
  LOG_ERROR << "[ERROR] " << msg;
}

inline void LogCritical(const std::string& msg) {
  LOG_ERROR << "[CRITICAL] " << msg;
}

BenchmarkManager::BenchmarkManager(QObject* parent)
    : QObject(parent), m_currentProcessId(0), saveToFile(true),
      m_finalWriteDone(false), m_firstWriteNeeded(true), m_benchmarkHash(""),
      m_diskTracker(std::make_unique<DiskPerformanceTracker>()),
      m_lastPdhMetricsLog(std::chrono::steady_clock::now()) {
  m_stateTracker = std::make_unique<BenchmarkStateTracker>();
  m_resultFileManager = std::make_unique<BenchmarkResultFileManager>();
  
  // Set up callbacks for benchmark detection (start from log pattern, end from timer)
  m_stateTracker->setBenchmarkStartCallback([this]() {
    LogCritical("Benchmark start signal received (log-based detection)");
    handleBenchmarkStart();
  });
  
  m_stateTracker->setBenchmarkEndCallback([this]() {
    LogCritical("Benchmark end signal received (timer-based: " + std::to_string(static_cast<int>(BenchmarkConstants::TARGET_BENCHMARK_DURATION)) + "s auto-end)");
    handleBenchmarkEnd();
  });
  currentBenchmarkState = BenchmarkStateTracker::State::OFF;
  emit benchmarkStateChanged(
    "<font color='#FFFFFF'>Benchmark: </font><font color='#FFFFFF'>OFF</font>");
  instance = this;

  m_cpuKernelTracker = std::make_unique<CPUKernelMetricsTracker>();

  try {
    LogCritical("[INIT] Creating PDH interface");
    m_pdhInterface = PdhInterface::createOptimizedForBenchmarking(std::chrono::milliseconds(BenchmarkConstants::METRICS_COLLECTION_INTERVAL_MS));
    if (!m_pdhInterface) {
      LogError("[INIT] Failed to create PdhInterface");
    } else {
      LogCritical("[INIT] PDH interface created successfully");
      if (m_pdhInterface->start()) {
        LogCritical("[INIT] PDH interface started successfully");
      } else {
        LogError("[INIT] Failed to start PDH interface");
      }
    }
  } catch (const std::exception& e) {
    LogError("[INIT] Exception during PdhInterface creation: " + std::string(e.what()));
    m_pdhInterface.reset();
  } catch (...) {
    LogError("[INIT] Unknown exception during PdhInterface creation");
    m_pdhInterface.reset();
  }
}

BenchmarkManager::~BenchmarkManager() {
  // Null out global instance and set safety flags
  if (instance == this) {
    instance = nullptr;
  }
  
  m_benchmarkEndDetected.store(true, std::memory_order_release);
  m_cleanupDone.store(true, std::memory_order_release);
  m_stopBenchmarkCalled.store(true, std::memory_order_release);
  std::atomic_thread_fence(std::memory_order_seq_cst);
  
  // Emergency thread join if needed
  if (benchmarkThread.joinable()) {
    try {
      benchmarkThread.join();
    } catch (const std::exception& e) {
      LogError("Exception joining thread in destructor: " + std::string(e.what()));
    }
  }

  // Clean up components
  if (m_stateTracker) {
    try {
      m_stateTracker->cleanup();
      m_stateTracker->setBenchmarkStartCallback(nullptr);
      m_stateTracker->setBenchmarkEndCallback(nullptr);
      m_stateTracker.reset();
    } catch (const std::exception& e) {
      LogError("Exception cleaning StateTracker: " + std::string(e.what()));
    }
  }

  if (m_demoManager) {
    m_demoManager.reset();
  }

  if (m_currentProcessId != 0) {
    try {
      stopBenchmark();
    } catch (const std::exception& e) {
      LogError("Exception in destructor stopBenchmark: " + std::string(e.what()));
    }
  }

  if (m_pdhInterface && !m_cleanupDone.load()) {
    try {
      m_pdhInterface->stop();
      m_pdhInterface.reset();
    } catch (const std::exception& e) {
      LogError("Exception cleaning PDH: " + std::string(e.what()));
    }
  }

  // Close result file manager if it exists
  if (m_resultFileManager) {
    m_resultFileManager->closeFile();
  }
}

class RateLimitedLog {
 private:
  std::chrono::steady_clock::time_point lastLog;
  std::chrono::milliseconds interval;

 public:
  RateLimitedLog(int intervalMs = 1000)
      : lastLog(std::chrono::steady_clock::now()), interval(intervalMs) {}

  template <typename T> void log(const T& msg) {
    auto now = std::chrono::steady_clock::now();
    if (now - lastLog >= interval) {
      LOG_INFO << msg;
      lastLog = now;
    }
  }
};

// Enhanced logging system for benchmark operations
class BenchmarkLogger {
private:
  static inline int m_logSampleCount = 0;
  static inline std::chrono::steady_clock::time_point m_benchmarkStartTime;
  static inline std::chrono::steady_clock::time_point m_lastStatusLog;
  static inline constexpr int MAX_INITIAL_SAMPLES = 3;
  static inline constexpr int STATUS_INTERVAL_SECONDS = 10;
  
public:
  static void resetForNewBenchmark() {
    m_logSampleCount = 0;
    m_benchmarkStartTime = std::chrono::steady_clock::now();
    m_lastStatusLog = m_benchmarkStartTime;
    LOG_INFO << "[Benchmark] Starting new benchmark (limited logging)";
  }
  
  static bool shouldLogSample() {
    return m_logSampleCount < MAX_INITIAL_SAMPLES;
  }
  
  static void logSample(const std::string& message) {
    if (shouldLogSample()) {
      m_logSampleCount++;
      LOG_INFO << "[Sample " << m_logSampleCount << "/" << MAX_INITIAL_SAMPLES << "] " 
                << message;
    }
  }
  
  static bool shouldLogStatus() {
    auto now = std::chrono::steady_clock::now();
    auto elapsed = std::chrono::duration_cast<std::chrono::seconds>(now - m_lastStatusLog);
    if (elapsed.count() >= STATUS_INTERVAL_SECONDS) {
      m_lastStatusLog = now;
      return true;
    }
    return false;
  }
  
  static void logStatus(const std::string& message) {
    auto now = std::chrono::steady_clock::now();
    auto elapsed = std::chrono::duration_cast<std::chrono::seconds>(now - m_benchmarkStartTime);
    LOG_INFO << "[Status @ " << elapsed.count() << "s] " << message;
  }
  
  static void logStateChange(const std::string& message) {
    auto now = std::chrono::steady_clock::now();
    auto elapsed = std::chrono::duration_cast<std::chrono::seconds>(now - m_benchmarkStartTime);
    LOG_INFO << "[State @ " << elapsed.count() << "s] " << message;
  }
};

bool InitializeTraceProperties(EVENT_TRACE_PROPERTIES* props) {
  ZeroMemory(props, sizeof(EVENT_TRACE_PROPERTIES));
  props->Wnode.BufferSize = sizeof(EVENT_TRACE_PROPERTIES);
  props->LogFileMode = EVENT_TRACE_REAL_TIME_MODE;
  return true;
}

bool KillExistingEtwSession(const std::wstring& sessionName) {
  EVENT_TRACE_PROPERTIES props = {};
  props.Wnode.BufferSize = sizeof(EVENT_TRACE_PROPERTIES);
  ULONG status =
    ControlTraceW(0, sessionName.c_str(), &props, EVENT_TRACE_CONTROL_STOP);
  return (status == ERROR_SUCCESS || status == ERROR_WMI_INSTANCE_NOT_FOUND);
}

bool IsCheckmarkPresentMonSessionKeyName(const wchar_t* keyName) {
  if (!keyName) {
    return false;
  }

  const wchar_t* kPrefix = L"PresentMon_Session_";
  const size_t prefixLen = wcslen(kPrefix);
  if (wcsncmp(keyName, kPrefix, prefixLen) != 0) {
    return false;
  }

  const wchar_t* p = keyName + prefixLen;
  if (*p == L'\0') {
    return false;
  }

  // Only delete keys that follow the exact session naming convention used by this app:
  // "PresentMon_Session_<digits>".
  for (; *p != L'\0'; ++p) {
    if (*p < L'0' || *p > L'9') {
      return false;
    }
  }

  return true;
}

bool cleanupRegistryEntries() {
  HKEY hKey;
  bool success = true;
  if (RegOpenKeyExW(HKEY_LOCAL_MACHINE,
                    L"SYSTEM\\CurrentControlSet\\Control\\WMI\\Autologger", 0,
                    KEY_READ | KEY_WRITE, &hKey) == ERROR_SUCCESS) {

    wchar_t keyName[MAX_PATH];
    DWORD index = 0;
    DWORD nameSize = MAX_PATH;

    while (RegEnumKeyExW(hKey, index++, keyName, &nameSize, NULL, NULL, NULL,
                         NULL) == ERROR_SUCCESS) {
      if (IsCheckmarkPresentMonSessionKeyName(keyName)) {
        if (RegDeleteKeyW(hKey, keyName) != ERROR_SUCCESS) {
          success = false;
        }
      }
      nameSize = MAX_PATH;
    }
    RegCloseKey(hKey);
  }
  return success;
}

void RegisterActiveSession(const std::wstring& sessionName) {
  std::lock_guard<std::mutex> lock(g_sessionMutex);
  g_activeSessions.insert(sessionName);
}

void UnregisterActiveSession(const std::wstring& sessionName) {
  std::lock_guard<std::mutex> lock(g_sessionMutex);
  g_activeSessions.erase(sessionName);
}

void OnMetricsUpdate(uint32_t processId, const PM_METRICS* metrics) {
  // Thread-safe instance access
  BenchmarkManager* localInstance = instance;
  if (!localInstance || !metrics) {
    return;
  }
  
  // Early exit if shutting down
  if (localInstance->m_cleanupDone.load(std::memory_order_acquire) ||
      localInstance->m_benchmarkEndDetected.load(std::memory_order_acquire) ||
      localInstance->m_stopBenchmarkCalled.load(std::memory_order_acquire)) {
    return;
  }

  try {
    // Store frame time data for percentile calculations (this still needs to accumulate)
    localInstance->accumulateMetrics(*metrics);

    // Update PM cache with latest ETW metrics
    {
      std::lock_guard<std::mutex> lock(localInstance->pmMutex);
      localInstance->pmCache.fps = metrics->fps;
      localInstance->pmCache.frameTime = metrics->frametime;
      localInstance->pmCache.gpuRenderTime = metrics->gpuRenderTime;
      localInstance->pmCache.cpuRenderTime = metrics->cpuRenderTime;
      localInstance->pmCache.highestFrameTime = metrics->maxFrameTime;
      localInstance->pmCache.highest5PctFrameTime = metrics->frameTime95Percentile;  // 95th percentile = highest 5% frametime
      localInstance->pmCache.highestGpuTime = metrics->maxGpuRenderTime;
      localInstance->pmCache.highestCpuTime = metrics->maxCpuRenderTime;
      localInstance->pmCache.fpsVariance = metrics->frameTimeVariance;
      localInstance->pmCache.destWidth = metrics->destWidth;
      localInstance->pmCache.destHeight = metrics->destHeight;
      localInstance->pmCache.presentCount = metrics->frameCount; // Use actual frame count from 1s window, not callback count
      
      // Calculate low FPS percentiles from frame time percentiles (per-second values from PresentMon)
      localInstance->pmCache.lowFps1Percent = (metrics->frameTime99Percentile > 0) ? 
        1000.0f / metrics->frameTime99Percentile : 0.0f;
      localInstance->pmCache.lowFps5Percent = (metrics->frameTime95Percentile > 0) ? 
        1000.0f / metrics->frameTime95Percentile : 0.0f;
      localInstance->pmCache.lowFps05Percent = (metrics->frameTime995Percentile > 0) ? 
        1000.0f / metrics->frameTime995Percentile : 0.0f;
        
      localInstance->pmCache.lastTimestamp = std::chrono::steady_clock::now();
    }
    
    // State updates are handled in the main benchmark loop with complete BenchmarkDataPoint
  } catch (const std::exception& e) {
    LogError("Exception in OnMetricsUpdate: " + std::string(e.what()));
  }
}

// BenchmarkSegment and findValidBenchmarkSegments removed - unused function

uint32_t BenchmarkManager::getProcessIdByName(const QString& processName) {
  if (processName.compare("RustClient.exe", Qt::CaseInsensitive) == 0) {
    HWND hwnd = FindWindowW(NULL, L"Rust");
    if (hwnd) {
      DWORD processId = 0;
      GetWindowThreadProcessId(hwnd, &processId);
      if (processId > 0) {
        return processId;
      }
    }

    hwnd = FindWindowW(NULL, NULL);
    while (hwnd) {
      DWORD processId = 0;
      GetWindowThreadProcessId(hwnd, &processId);

      if (processId > 0) {
        WCHAR windowTitle[256];
        if (GetWindowTextW(hwnd, windowTitle,
                           sizeof(windowTitle) / sizeof(WCHAR)) > 0) {
          if (wcsstr(windowTitle, L"Rust") != nullptr) {
            return processId;
          }
        }
      }
      hwnd = GetWindow(hwnd, GW_HWNDNEXT);
    }

    DWORD foregroundProcessId = 0;
    hwnd = GetForegroundWindow();
    if (hwnd) {
      GetWindowThreadProcessId(hwnd, &foregroundProcessId);
      return foregroundProcessId;
    }
  }

  LogError("Could not find process: " + processName.toStdString());
  return 0;
}

bool BenchmarkManager::restartWithElevation() {
  wchar_t szPath[MAX_PATH];
  if (GetModuleFileNameW(NULL, szPath, ARRAYSIZE(szPath))) {
    SHELLEXECUTEINFOW sei = {sizeof(sei)};
    sei.lpVerb = L"runas";
    sei.lpFile = szPath;
    sei.hwnd = NULL;
    sei.nShow = SW_NORMAL;

    if (!ShellExecuteExW(&sei)) {
      DWORD dwError = GetLastError();
      if (dwError == ERROR_CANCELLED) {
        emit benchmarkError("User declined elevation request");
      } else {
        emit benchmarkError("Failed to restart with elevated privileges");
      }
      return false;
    }
    QCoreApplication::quit();
    return true;
  }
  return false;
}

bool BenchmarkManager::startBenchmark(const QString& processName,
                                      int durationSeconds) {
  // Make sure DemoFileManager is created if it doesn't exist
  if (!m_demoManager) {
    m_demoManager = std::make_unique<DemoFileManager>(this);
    connect(m_demoManager.get(), &DemoFileManager::validationError, this,
            &BenchmarkManager::benchmarkError);
  }

  // Check prerequisites first
  if (!m_demoManager->checkBenchmarkPrerequisites(processName)) {
    return false;
  }

  // Existing startBenchmark code continues here...
  LogCritical("[INIT] Resetting benchmark state for new run");
  
  // Reset cleanup flags for new benchmark
  m_cleanupDone.store(false, std::memory_order_release);
  m_stopBenchmarkCalled.store(false, std::memory_order_release);
  m_benchmarkEndDetected.store(false, std::memory_order_release);
  m_nvencUsageActive.store(false, std::memory_order_release);
  emit nvencUsageDetected(false);  // Reset any lingering NVENC warning banner
  
  LogCritical("[INIT] Cleanup flags reset: cleanupDone=" + std::to_string(m_cleanupDone.load()) + 
              ", stopCalled=" + std::to_string(m_stopBenchmarkCalled.load()) + 
              ", endDetected=" + std::to_string(m_benchmarkEndDetected.load()));
  
  if (m_resultFileManager) {
    m_resultFileManager->closeFile();
  }

  m_outputFilename.clear();
  m_finalWriteDone = false;
  m_firstWriteNeeded = true;

  QString timestamp =
    QDateTime::currentDateTime().toString("yyyy-MM-dd_HH-mm-ss");

  // Initialize UserSystemProfile for this benchmark
  auto& userProfile = SystemMetrics::UserSystemProfile::getInstance();
  if (!userProfile.isInitialized()) {
    userProfile.initialize();
  }

  // Get the user+system identifier and add it to the system info
  std::string systemIdentifier = userProfile.getCombinedIdentifier();

  // Generate new benchmark hash using BenchmarkSpecsFileManager
  m_benchmarkHash = BenchmarkSpecsFileManager::generateNewBenchmarkHash();

  // Store the user system identifier for this benchmark run
  m_userSystemId = QString::fromStdString(systemIdentifier);

  if (m_benchmarkHash.isEmpty()) {
    LogError("Failed to generate benchmark hash");
    emit benchmarkError(
      "Failed to create benchmark file: hash generation failed");
    return false;
  }

  m_outputFilename = QString("%1_%2.csv").arg(timestamp).arg(m_benchmarkHash);

  QDir().mkpath("benchmark_results");

  QString specsFilename = "benchmark_results/" + m_outputFilename;
  specsFilename.replace(".csv", "_specs.txt");
  BenchmarkSpecsFileManager::saveSystemSpecsToFile(specsFilename, true);


  {
    std::lock_guard<std::mutex> lock(fpsValuesMutex);
    allFpsSamples.clear();
  }

  // Clear frame time points for cumulative percentile calculations
  {
    std::lock_guard<std::mutex> lock(frameTimesMutex);
    allFrameTimePoints.clear();
    frameTimeHistogram.clear();
  }

  {
    std::lock_guard<std::recursive_mutex> lock(dataMutex);
    allData.clear();
    currentData = BenchmarkDataPoint();
  }

  benchmarkStartTime = std::chrono::steady_clock::now();

  QString warnings = BenchmarkSpecsFileManager::getSystemWarnings();
  if (!warnings.isEmpty()) {
    LOG_WARN << "\nSystem Configuration Warnings:\n"
              << warnings.toStdString();
    emit benchmarkWarning(warnings);
  }

  BOOL isElevated = FALSE;
  HANDLE hToken = NULL;
  if (OpenProcessToken(GetCurrentProcess(), TOKEN_QUERY, &hToken)) {
    TOKEN_ELEVATION elevation;
    DWORD size = sizeof(TOKEN_ELEVATION);
    if (GetTokenInformation(hToken, TokenElevation, &elevation,
                            sizeof(elevation), &size)) {
      isElevated = elevation.TokenIsElevated;
    }
    CloseHandle(hToken);
  }

  if (!isElevated) {
    LogCritical("Requesting elevation");
    return restartWithElevation();
  }

  LogCritical("Starting benchmark for: " + processName.toStdString());

  cleanupExistingETWSessions();

  // Make sure trackers are fully recreated for each benchmark run
  if (m_diskTracker) {
    m_diskTracker.reset();
  }
  m_diskTracker = std::make_unique<DiskPerformanceTracker>();

  if (m_cpuKernelTracker) {
    m_cpuKernelTracker.reset();
  }
  m_cpuKernelTracker = std::make_unique<CPUKernelMetricsTracker>();

  // Add a small delay to ensure Windows has time to clean up any lingering ETW
  // sessions
  std::this_thread::sleep_for(std::chrono::milliseconds(500));

  auto status = PM_Initialize();
  if (status != PM_STATUS::PM_SUCCESS) {
    QString errorMsg = QString("Failed to initialize PresentMon (Status: %1)")
                         .arg(static_cast<int>(status));
    LogError(errorMsg.toStdString());
    emit benchmarkError(errorMsg);
    return false;
  }

  uint32_t processId = getProcessIdByName(processName);
  if (processId == 0) {
    QString errorMsg = QString("Process not found: %1").arg(processName);
    LogError(errorMsg.toStdString());
    emit benchmarkError(errorMsg);
    cleanup();
    return false;
  }

  if (!m_stateTracker->initialize(processId)) {
    emit benchmarkError("Failed to initialize state tracker");
    cleanup();
    return false;
  }

  // Ensure callbacks are (re)registered for this run in case a prior cleanup cleared them
  if (m_stateTracker) {
    m_stateTracker->setBenchmarkStartCallback([this]() {
      LogCritical("Benchmark start signal received (log-based detection)");
      handleBenchmarkStart();
    });
    m_stateTracker->setBenchmarkEndCallback([this]() {
      LogCritical(
        "Benchmark end signal received (timer-based: " +
        std::to_string(static_cast<int>(BenchmarkConstants::TARGET_BENCHMARK_DURATION)) +
        "s auto-end)");
      handleBenchmarkEnd();
    });
  }


  std::wstring sessionName =
    L"PresentMon_Session_" + std::to_wstring(processId);
  RegisterActiveSession(sessionName);

  PM_SetMetricsCallback(OnMetricsUpdate);

  status = PM_StartMonitoring(processId, BenchmarkConstants::METRICS_COLLECTION_INTERVAL_MS);
  if (status != PM_STATUS::PM_SUCCESS) {
    UnregisterActiveSession(sessionName);
    QString errorMsg = QString("Failed to start monitoring (Status: %1)")
                         .arg(static_cast<int>(status));
    LogError(errorMsg.toStdString());
    emit benchmarkError(errorMsg);
    cleanup();
    return false;
  }

  startTime = std::chrono::steady_clock::now();

  // Since we get most metrics from PDH now, we only need process-specific memory data
  // which we can get directly from the Windows API if needed
  m_currentProcessId = processId;
  m_shouldStop = false;



  if (m_stateTracker) {
    currentBenchmarkState = BenchmarkStateTracker::State::WAITING;
    emit benchmarkStateChanged("<font color='#FFFFFF'>Benchmark: </font><font "
                               "color='#FFD700'>Waiting...</font>");
  }


  m_gpuMetrics = std::make_unique<NvidiaMetricsCollector>();
  connect(m_gpuMetrics.get(), &NvidiaMetricsCollector::metricsUpdated, this,
          [this](const NvidiaGPUMetrics& metrics) {
            // Update NVIDIA cache with all available metrics
            {
              std::lock_guard<std::mutex> lock(nvMutex);

              // Basic metrics
              nvCache.gpuTemperature = metrics.temperature;
              nvCache.gpuCoreUtilization = metrics.utilization;
              nvCache.gpuMemoryUtilization = metrics.memoryUtilization;
              nvCache.gpuPowerUsage = metrics.powerUsage;
              nvCache.gpuMemoryUsage = static_cast<float>(metrics.usedMemory) / static_cast<float>(metrics.totalMemory) * 100.0f;
              nvCache.gpuMemUsed = metrics.usedMemory;     // Store raw used memory in bytes
              nvCache.gpuMemTotal = metrics.totalMemory;   // Store raw total memory in bytes
              
              // Clock speeds
              nvCache.gpuClock = metrics.clockSpeed;
              nvCache.gpuMemClock = metrics.memoryClock;
              
              // Fan speed
              nvCache.gpuFanSpeed = metrics.fanSpeed;
              
              // Advanced utilization metrics
              nvCache.gpuSmUtilization = metrics.smUtilization;
              nvCache.gpuMemBandwidthUtil = metrics.memoryBandwidthUtilization;
              
              // PCIe throughput
              nvCache.gpuPcieRxThroughput = metrics.pcieRxThroughput;
              nvCache.gpuPcieTxThroughput = metrics.pcieTxThroughput;
              
              // Video encoder/decoder utilization
              nvCache.gpuNvdecUtil = metrics.nvdecUtilization;
              nvCache.gpuNvencUtil = metrics.nvencUtilization;
              
              // Throttling status
              nvCache.gpuThrottling = metrics.throttling;
              
              nvCache.lastTimestamp = std::chrono::steady_clock::now();
            }

            // Detect screen capture/recording via NVENC utilization to warn about skewed FPS metrics
            constexpr unsigned int NVENC_USAGE_WARNING_THRESHOLD = 5;  // % utilization that typically indicates capture/streaming
            const bool nvencActiveNow =
              metrics.nvencUtilization >= NVENC_USAGE_WARNING_THRESHOLD;
            const bool wasActive = m_nvencUsageActive.exchange(nvencActiveNow);
            if (nvencActiveNow != wasActive) {
              emit nvencUsageDetected(nvencActiveNow);
            }
          });

  m_gpuMetrics->startCollecting(BenchmarkConstants::METRICS_COLLECTION_INTERVAL_MS);

  if (m_diskTracker) {
    m_diskTracker->startTracking();
  }

  if (m_cpuKernelTracker) {
    m_cpuKernelTracker->startTracking();
  }

  // Recreate PDH interface if it was destroyed in previous cleanup
  if (!m_pdhInterface) {
    try {
      LogCritical("[INIT] Recreating PDH interface for new benchmark");
      m_pdhInterface = PdhInterface::createOptimizedForBenchmarking(std::chrono::milliseconds(BenchmarkConstants::METRICS_COLLECTION_INTERVAL_MS));
      if (!m_pdhInterface) {
        LogError("[INIT] Failed to recreate PdhInterface");
      } else {
        LogCritical("[INIT] PDH interface recreated successfully");
      }
    } catch (const std::exception& e) {
      LogError("[INIT] Exception during PDH interface recreation: " + std::string(e.what()));
      m_pdhInterface.reset();
    }
  }
  
  if (m_pdhInterface) {
    try {
      LogCritical("[INIT] Starting PDH interface");
      bool pdhStarted = m_pdhInterface->start();
      if (pdhStarted) {
        LogCritical("[INIT] PDH interface started successfully");
        
        // Wait a moment for metrics to be populated (PDH needs time to collect initial data)
        std::this_thread::sleep_for(std::chrono::milliseconds(1500));
        
        
      } else {
        LogError("[INIT] Failed to start PdhInterface - trying fallback");
        
        // Try minimal initialization as fallback
        m_pdhInterface.reset();
        LogCritical("[INIT] Trying minimal PDH interface");
        m_pdhInterface = PdhInterface::createMinimal(std::chrono::milliseconds(BenchmarkConstants::METRICS_COLLECTION_INTERVAL_MS));
        if (!m_pdhInterface || !m_pdhInterface->start()) {
          LogError("[INIT] All PdhInterface initialization methods failed");
          m_pdhInterface.reset();
        } else {
          LogCritical("[INIT] Minimal PDH interface started successfully");
        }
      }
    } catch (const std::exception& e) {
      LogError("[INIT] Exception during PdhInterface start: " + std::string(e.what()));
      m_pdhInterface.reset();
    }
  } else {
    LogError("[INIT] PdhInterface not available - limited metrics only");
  }

  benchmarkThread = std::thread([this, processId, durationSeconds]() {
    try {
      BenchmarkLogger::resetForNewBenchmark();
      
      // Enable CSV data collection during the benchmark
      LogCritical("Enabling saveToFile for benchmark data collection");
      saveToFile = true;
      
      int noDataCount = 0;
      std::vector<BenchmarkDataPoint> batchBuffer;

      for (int i = 0; i <= durationSeconds && !m_shouldStop; ++i) {
        emit benchmarkProgress(i * 100 / durationSeconds);

        // Only log every 10 seconds or for first 3 samples
        if (BenchmarkLogger::shouldLogStatus() || BenchmarkLogger::shouldLogSample()) {
          if (BenchmarkLogger::shouldLogSample()) {
            BenchmarkLogger::logSample("Second " + std::to_string(i) + "/" + std::to_string(durationSeconds));
          } else {
            BenchmarkLogger::logStatus("Progress: " + std::to_string(i) + "/" + std::to_string(durationSeconds) + " seconds");
          }
        }

        PM_METRICS latestMetrics;
        std::vector<PM_METRICS> allNewMetrics;
        PM_STATUS status = PM_GetMetrics(processId, &latestMetrics, &allNewMetrics);
        
        // Debug removed - CSV-SNAPSHOT now logs during RUNNING state only
        
        if (status == PM_STATUS::PM_SUCCESS) {
          if (allNewMetrics.empty() && latestMetrics.fps == 0 &&
              latestMetrics.gpuRenderTime == 0) {
            noDataCount++;
            if (noDataCount > 5 && BenchmarkLogger::shouldLogStatus()) {
              BenchmarkLogger::logStatus("WARNING: No valid ETW data for " + std::to_string(noDataCount) + " seconds");
            }
          } else {
            noDataCount = 0;
            if (BenchmarkLogger::shouldLogSample()) {
              BenchmarkLogger::logSample(
                "ETW: FPS=" + std::to_string(latestMetrics.fps) + 
                ", Frame=" + std::to_string(latestMetrics.frametime) + "ms");
            }
          }
        } else if (BenchmarkLogger::shouldLogStatus()) {
          BenchmarkLogger::logStatus("PM_GetMetrics failed with status: " + std::to_string(static_cast<int>(status)));
        }

        // Ensure PDH metrics are collected every second regardless of other collectors
        // Update PDH cache FIRST to get fresh data for this sample
        try {
          accumulatePdhMetrics();
        } catch (const std::exception& e) {
          LogError("PDH collection failed: " + std::string(e.what()));
          // Continue with other metrics - don't let PDH failure stop the benchmark
        }
        
        // Build coherent sample from all provider caches
        BenchmarkDataPoint sample;
        
        // Copy from PM cache (ETW frame data) - CSV SAMPLING VERSION
        {
          std::lock_guard<std::mutex> lock(pmMutex);
          sample.fps = pmCache.fps;
          sample.frameTime = pmCache.frameTime;
          sample.gpuRenderTime = pmCache.gpuRenderTime;
          sample.cpuRenderTime = pmCache.cpuRenderTime;
          sample.highestFrameTime = pmCache.highestFrameTime;
          sample.highest5PctFrameTime = pmCache.highest5PctFrameTime;    // Per-second highest 5% frametime for CSV
          sample.highestGpuTime = pmCache.highestGpuTime;
          sample.highestCpuTime = pmCache.highestCpuTime;
          sample.fpsVariance = pmCache.fpsVariance;
          // For CSV export, use per-second percentiles from PresentMon (RESETS EVERY SECOND)
          // These values come from PresentMon's built-in rolling 1-second window calculations
          sample.lowFps1Percent = pmCache.lowFps1Percent;   // Based on worst 1% of frames in this 1-second window
          sample.lowFps5Percent = pmCache.lowFps5Percent;   // Based on worst 5% of frames in this 1-second window  
          sample.lowFps05Percent = pmCache.lowFps05Percent; // Based on worst 0.5% of frames in this 1-second window
          sample.destWidth = pmCache.destWidth;
          sample.destHeight = pmCache.destHeight;
          sample.presentCount = pmCache.presentCount;
        }
        
        // Copy from PDH cache (system metrics) - NOW WITH FRESH DATA
        {
          std::lock_guard<std::mutex> lock(pdhMutex);
          sample.procProcessorTime = pdhCache.procProcessorTime;
          sample.procUserTime = pdhCache.procUserTime;
          sample.procPrivilegedTime = pdhCache.procPrivilegedTime;
          sample.procIdleTime = pdhCache.procIdleTime;
          sample.procActualFreq = pdhCache.procActualFreq;
          sample.cpuInterruptsPerSec = pdhCache.cpuInterruptsPerSec;
          sample.cpuDpcTime = pdhCache.cpuDpcTime;
          sample.cpuInterruptTime = pdhCache.cpuInterruptTime;
          sample.cpuDpcsQueuedPerSec = pdhCache.cpuDpcsQueuedPerSec;
          sample.cpuDpcRate = pdhCache.cpuDpcRate;
          sample.cpuC1Time = pdhCache.cpuC1Time;
          sample.cpuC2Time = pdhCache.cpuC2Time;
          sample.cpuC3Time = pdhCache.cpuC3Time;
          sample.cpuC1TransitionsPerSec = pdhCache.cpuC1TransitionsPerSec;
          sample.cpuC2TransitionsPerSec = pdhCache.cpuC2TransitionsPerSec;
          sample.cpuC3TransitionsPerSec = pdhCache.cpuC3TransitionsPerSec;
          sample.availableMemoryMB = pdhCache.availableMemoryMB;
          sample.memoryLoad = pdhCache.memoryLoad;
          sample.memoryCommittedBytes = pdhCache.memoryCommittedBytes;
          sample.memoryCommitLimit = pdhCache.memoryCommitLimit;
          sample.memoryFaultsPerSec = pdhCache.memoryFaultsPerSec;
          sample.memoryPagesPerSec = pdhCache.memoryPagesPerSec;
          sample.memoryPoolNonPagedBytes = pdhCache.memoryPoolNonPagedBytes;
          sample.memoryPoolPagedBytes = pdhCache.memoryPoolPagedBytes;
          sample.memorySystemCodeBytes = pdhCache.memorySystemCodeBytes;
          sample.memorySystemDriverBytes = pdhCache.memorySystemDriverBytes;
          sample.ioReadRateMBs = pdhCache.ioReadRateMBs;
          sample.ioWriteRateMBs = pdhCache.ioWriteRateMBs;
          sample.diskReadsPerSec = pdhCache.diskReadsPerSec;
          sample.diskWritesPerSec = pdhCache.diskWritesPerSec;
          sample.diskTransfersPerSec = pdhCache.diskTransfersPerSec;
          sample.diskBytesPerSec = pdhCache.diskBytesPerSec;
          sample.diskAvgReadQueueLength = pdhCache.diskAvgReadQueueLength;
          sample.diskAvgWriteQueueLength = pdhCache.diskAvgWriteQueueLength;
          sample.diskAvgQueueLength = pdhCache.diskAvgQueueLength;
          sample.diskAvgReadTime = pdhCache.diskAvgReadTime;
          sample.diskAvgWriteTime = pdhCache.diskAvgWriteTime;
          sample.diskAvgTransferTime = pdhCache.diskAvgTransferTime;
          sample.diskPercentTime = pdhCache.diskPercentTime;
          sample.diskPercentReadTime = pdhCache.diskPercentReadTime;
          sample.diskPercentWriteTime = pdhCache.diskPercentWriteTime;
          sample.contextSwitchesPerSec = pdhCache.contextSwitchesPerSec;
          sample.systemProcessorQueueLength = pdhCache.systemProcessorQueueLength;
          sample.systemProcesses = pdhCache.systemProcesses;
          sample.systemThreads = pdhCache.systemThreads;
          sample.pdhInterruptsPerSec = pdhCache.pdhInterruptsPerSec;
          // Per-core data is already fresh since accumulatePdhMetrics was called above
          sample.perCoreCpuUsagePdh = pdhCache.perCoreCpuUsage;
          sample.perCoreActualFreq = pdhCache.perCoreActualFreq;
        }
        
        // Copy from NV cache (GPU metrics) - COMPLETE VERSION WITH ALL METRICS
        {
          std::lock_guard<std::mutex> lock(nvMutex);
          // Basic GPU metrics
          sample.gpuTemp = nvCache.gpuTemperature;
          sample.gpuUtilization = nvCache.gpuCoreUtilization;
          sample.gpuMemUtilization = nvCache.gpuMemoryUtilization;
          sample.gpuPower = nvCache.gpuPowerUsage;
          sample.gpuMemUsed = nvCache.gpuMemUsed;         // Raw used memory in bytes
          sample.gpuMemTotal = nvCache.gpuMemTotal;       // Raw total memory in bytes
          
          // Clock speeds - THESE WERE MISSING!
          sample.gpuClock = nvCache.gpuClock;
          sample.gpuMemClock = nvCache.gpuMemClock;
          
          // Fan speed - THIS WAS MISSING!
          sample.gpuFanSpeed = nvCache.gpuFanSpeed;
          
          // Advanced utilization metrics - THESE WERE MISSING!
          sample.gpuSmUtilization = nvCache.gpuSmUtilization;
          sample.gpuMemBandwidthUtil = nvCache.gpuMemBandwidthUtil;
          
          // PCIe throughput - THESE WERE MISSING!
          sample.gpuPcieRxThroughput = nvCache.gpuPcieRxThroughput;
          sample.gpuPcieTxThroughput = nvCache.gpuPcieTxThroughput;
          
          // Video encoder/decoder utilization - THESE WERE MISSING!
          sample.gpuNvdecUtil = nvCache.gpuNvdecUtil;
          sample.gpuNvencUtil = nvCache.gpuNvencUtil;
          
          // Throttling status - THIS WAS MISSING!
          sample.gpuThrottling = nvCache.gpuThrottling;
        }
        
        
        // Update data from ETW trackers
        if (m_diskTracker) {
          try {
            m_diskTracker->updateBenchmarkData(sample);
          } catch (const std::exception& e) {
            LogError("Disk tracker updateBenchmarkData failed: " + std::string(e.what()));
          }
        }
        
        if (m_cpuKernelTracker) {
          try {
            m_cpuKernelTracker->updateBenchmarkData(sample);
          } catch (const std::exception& e) {
            LogError("CPU kernel tracker updateBenchmarkData failed: " + std::string(e.what()));
          }
        }
        
        {
          std::lock_guard<std::recursive_mutex> lock(dataMutex);
          
          // IMPORTANT: The 'sample' variable above contains PER-SECOND metrics from PresentMon.
          // This is exactly what we want for CSV export - per-second reset values.
          // The CSV file will show per-second percentiles that reset every second.
          
          // For UI display, emitUIMetrics() will create a separate sample with CUMULATIVE metrics
          // that accumulate ALL frametimes since benchmark start.
          
          // Update currentData as lastCommittedSample for compatibility  
          currentData = sample;
          
          if (BenchmarkLogger::shouldLogStatus()) {
            // Comprehensive metrics logging every 10 seconds - all CSV data
            std::string metricsLog = "[METRICS] ";
            
            // Core performance metrics
            metricsLog += "FPS: " + std::to_string(static_cast<int>(sample.fps)) + " ";
            metricsLog += "FrameTime: " + std::to_string(static_cast<int>(sample.frameTime)) + "ms ";
            metricsLog += "1%Low: " + std::to_string(static_cast<int>(sample.lowFps1Percent)) + " ";
            metricsLog += "0.1%Low: " + std::to_string(static_cast<int>(sample.lowFps05Percent)) + " | ";
            
            // CPU metrics
            metricsLog += "CPU: " + std::to_string(static_cast<int>(sample.procProcessorTime)) + "% ";
            metricsLog += "Freq: " + std::to_string(static_cast<int>(sample.procActualFreq)) + "MHz ";
            metricsLog += "Cores: " + std::to_string(sample.perCoreCpuUsagePdh.size());
            
            // Add per-core CPU data sample to metrics log
            if (!sample.perCoreCpuUsagePdh.empty()) {
              metricsLog += " [C0-3: ";
              for (size_t i = 0; i < std::min(sample.perCoreCpuUsagePdh.size(), size_t(4)); ++i) {
                if (i > 0) metricsLog += ",";
                metricsLog += std::to_string(static_cast<int>(sample.perCoreCpuUsagePdh[i])) + "%";
              }
              if (sample.perCoreCpuUsagePdh.size() > 4) metricsLog += "...";
              metricsLog += "]";
            } else {
              metricsLog += " [NO-DATA]";
            }
            
            // Add per-core frequency data sample
            if (!sample.perCoreActualFreq.empty()) {
              metricsLog += " [F0-3: ";
              for (size_t i = 0; i < std::min(sample.perCoreActualFreq.size(), size_t(4)); ++i) {
                if (i > 0) metricsLog += ",";
                metricsLog += std::to_string(static_cast<int>(sample.perCoreActualFreq[i])) + "MHz";
              }
              if (sample.perCoreActualFreq.size() > 4) metricsLog += "...";
              metricsLog += "]";
            } else {
              metricsLog += " [NO-FREQ]";
            }
            metricsLog += " | ";
            
            // Memory metrics
            metricsLog += "RAM: " + std::to_string(static_cast<int>(sample.availableMemoryMB)) + "MB ";
            metricsLog += "Load: " + std::to_string(static_cast<int>(sample.memoryLoad)) + "% ";
            metricsLog += "Cache: " + std::to_string(static_cast<int>(sample.memorySystemCodeBytes / (1024*1024))) + "MB | ";
            
            // GPU metrics
            metricsLog += "GPU: " + std::to_string(sample.gpuUtilization) + "% ";
            metricsLog += "Temp: " + std::to_string(sample.gpuTemp) + "C ";
            metricsLog += "VRAM: " + std::to_string(static_cast<int>(sample.gpuMemUsed / (1024*1024*1024))) + "GB | ";
            
            // I/O metrics
            metricsLog += "DiskR: " + std::to_string(static_cast<int>(sample.ioReadRateMBs)) + "MB/s ";
            metricsLog += "DiskW: " + std::to_string(static_cast<int>(sample.ioWriteRateMBs)) + "MB/s ";
            
            // System metrics
            metricsLog += "CtxSw: " + std::to_string(static_cast<int>(sample.contextSwitchesPerSec)) + "/s";
            
            BenchmarkLogger::logStatus(metricsLog);
          }
          
          // Check and log state tracker status
          if (m_stateTracker) {
            BenchmarkDataPoint stateData;
            stateData.procProcessorTime = sample.procProcessorTime;
            stateData.availableMemoryMB = sample.availableMemoryMB;
            stateData.fps = sample.fps;
            
            PM_METRICS stateMetrics{};
            stateMetrics.fps = sample.fps;
            stateMetrics.frametime = sample.frameTime;
            
            // Protect StateTracker access to prevent race conditions with callback
            BenchmarkStateTracker::State previousState = BenchmarkStateTracker::State::OFF;
            BenchmarkStateTracker::State newState = BenchmarkStateTracker::State::OFF;
            
            if (m_benchmarkEndDetected.load()) {
              previousState = newState = BenchmarkStateTracker::State::COOLDOWN;
            } else if (m_stateTracker) {
              try {
                previousState = m_stateTracker->getCurrentState();
                newState = m_stateTracker->updateState(stateMetrics, stateData);
              } catch (...) {
                LogError("Exception in StateTracker access");
                previousState = newState = BenchmarkStateTracker::State::COOLDOWN;
              }
            }
            
            if (newState != previousState) {
              std::string prevName = "UNKNOWN", newName = "UNKNOWN";
              switch (previousState) {
                case BenchmarkStateTracker::State::OFF: prevName = "OFF"; break;
                case BenchmarkStateTracker::State::WAITING: prevName = "WAITING"; break;
                case BenchmarkStateTracker::State::RUNNING: prevName = "RUNNING"; break;
                case BenchmarkStateTracker::State::COOLDOWN: prevName = "COOLDOWN"; break;
              }
              switch (newState) {
                case BenchmarkStateTracker::State::OFF: newName = "OFF"; break;
                case BenchmarkStateTracker::State::WAITING: newName = "WAITING"; break;
                case BenchmarkStateTracker::State::RUNNING: newName = "RUNNING"; break;
                case BenchmarkStateTracker::State::COOLDOWN: newName = "COOLDOWN"; break;
              }
              
              BenchmarkLogger::logStateChange("STATE TRANSITION: " + prevName + " -> " + newName);
              
              if (newState == BenchmarkStateTracker::State::RUNNING) {
                BenchmarkLogger::logStateChange("*** BENCHMARK STARTED - NOW COLLECTING CSV DATA ***");
              } else if (newState == BenchmarkStateTracker::State::COOLDOWN) {
                BenchmarkLogger::logStateChange("*** BENCHMARK ENDED - STOPPING CSV COLLECTION ***");
              }
            }
          }
        }

        std::this_thread::sleep_for(std::chrono::seconds(1));

        // Set timestamp and publish the coherent sample
        sample.timestamp = i;
        
        {
          std::lock_guard<std::recursive_mutex> lock(dataMutex);
          
          // Update lastCommittedSample with the completed sample
          lastCommittedSample = sample;
          
          BenchmarkLogger::logSample(
            "Data point " + std::to_string(i) + 
            " - CPU: " + std::to_string(sample.procProcessorTime) + "%" + 
            ", Memory: " + std::to_string(sample.availableMemoryMB) + " MB" +
            ", FPS: " + std::to_string(sample.fps));
          
          // *** SIMPLIFIED: Just check the current state - no complex StateTracker calls ***
          bool isRunningState = (currentBenchmarkState == BenchmarkStateTracker::State::RUNNING);
          
          if (BenchmarkLogger::shouldLogSample()) {
            LogCritical("Second " + std::to_string(i) + 
                       " - State: " + (isRunningState ? "RUNNING" : "NOT_RUNNING") + 
                       ", saveToFile: " + (saveToFile ? "true" : "false") + 
                       ", allData.size(): " + std::to_string(allData.size()) + 
                       ", batchBuffer.size(): " + std::to_string(batchBuffer.size()));
          }
          
          // Save per-second percentiles before overwriting with cumulative values  
          BenchmarkDataPoint csvSample = sample;  // Copy with per-second percentiles for CSV
          
          if (isRunningState) {
            allData.push_back(csvSample);  // Use per-second percentiles for CSV
            batchBuffer.push_back(csvSample);
            
            // Log CSV snapshot every 10 seconds during RUNNING state - this matches actual CSV data
            static std::chrono::steady_clock::time_point lastCsvSnapshot;
            static bool firstSnapshot = true;
            auto now = std::chrono::steady_clock::now();
            
            if (firstSnapshot || std::chrono::duration_cast<std::chrono::seconds>(now - lastCsvSnapshot).count() >= 10) {
              // This is the EXACT data being written to CSV
              std::string csvSnapshot = "[CSV-SNAPSHOT] ";
              
              // Frame metrics (from ETW) - Use csvSample to show actual CSV data
              csvSnapshot += "FPS:" + std::to_string(static_cast<int>(csvSample.fps * 10) / 10.0) + " ";
              csvSnapshot += "FrameT:" + std::to_string(static_cast<int>(csvSample.frameTime * 10) / 10.0) + "ms ";
              csvSnapshot += "MaxFT:" + std::to_string(static_cast<int>(csvSample.highestFrameTime * 10) / 10.0) + "ms ";
              csvSnapshot += "1%Low(PerSec):" + std::to_string(static_cast<int>(csvSample.lowFps1Percent * 10) / 10.0) + " ";
              csvSnapshot += "5%Low(PerSec):" + std::to_string(static_cast<int>(csvSample.lowFps5Percent * 10) / 10.0) + " ";
              csvSnapshot += "0.1%Low(PerSec):" + std::to_string(static_cast<int>(csvSample.lowFps05Percent * 10) / 10.0) + " ";
              csvSnapshot += "GPURender:" + std::to_string(static_cast<int>(csvSample.gpuRenderTime * 10) / 10.0) + "ms ";
              csvSnapshot += "CPURender:" + std::to_string(static_cast<int>(csvSample.cpuRenderTime * 10) / 10.0) + "ms | ";
              
              // CPU metrics (from PDH)
              csvSnapshot += "CPUTotal:" + std::to_string(static_cast<int>(sample.procProcessorTime * 10) / 10.0) + "% ";
              csvSnapshot += "CPUFreq:" + std::to_string(static_cast<int>(sample.procActualFreq)) + "MHz ";
              csvSnapshot += "UserTime:" + std::to_string(static_cast<int>(sample.procUserTime * 10) / 10.0) + "% ";
              csvSnapshot += "PrivTime:" + std::to_string(static_cast<int>(sample.procPrivilegedTime * 10) / 10.0) + "% ";
              csvSnapshot += "IdleTime:" + std::to_string(static_cast<int>(sample.procIdleTime * 10) / 10.0) + "% | ";
              
              // Memory metrics (from PDH)
              csvSnapshot += "MemAvail:" + std::to_string(static_cast<int>(sample.availableMemoryMB)) + "MB ";
              csvSnapshot += "MemLoad:" + std::to_string(static_cast<int>(sample.memoryLoad * 10) / 10.0) + "% ";
              csvSnapshot += "Committed:" + std::to_string(static_cast<int>(sample.memoryCommittedBytes / (1024*1024))) + "MB ";
              csvSnapshot += "CommitLim:" + std::to_string(static_cast<int>(sample.memoryCommitLimit / (1024*1024))) + "MB ";
              csvSnapshot += "PageFaults:" + std::to_string(static_cast<int>(sample.memoryFaultsPerSec)) + "/s | ";
              
              // GPU metrics (from NVIDIA)
              csvSnapshot += "GPU:" + std::to_string(sample.gpuUtilization) + "% ";
              csvSnapshot += "GPUTemp:" + std::to_string(sample.gpuTemp) + "C ";
              csvSnapshot += "GPUClock:" + std::to_string(sample.gpuClock) + "MHz ";
              csvSnapshot += "GPUMemClock:" + std::to_string(sample.gpuMemClock) + "MHz ";
              csvSnapshot += "VRAM:" + std::to_string(static_cast<int>((sample.gpuMemUsed / (1024.0 * 1024.0 * 1024.0)) * 10) / 10.0) + "GB ";
              csvSnapshot += "VRAMTotal:" + std::to_string(static_cast<int>((sample.gpuMemTotal / (1024.0 * 1024.0 * 1024.0)) * 10) / 10.0) + "GB ";
              csvSnapshot += "GPUPower:" + std::to_string(sample.gpuPower) + "mW ";
              csvSnapshot += "GPUFan:" + std::to_string(sample.gpuFanSpeed) + "% | ";
              
              // I/O metrics (from DiskTracker)
              csvSnapshot += "IOReadRate:" + std::to_string(static_cast<int>(sample.ioReadRateMBs * 10) / 10.0) + "MB/s ";
              csvSnapshot += "IOWriteRate:" + std::to_string(static_cast<int>(sample.ioWriteRateMBs * 10) / 10.0) + "MB/s ";
              csvSnapshot += "IOReadTotal:" + std::to_string(static_cast<int>(sample.ioReadMB * 10) / 10.0) + "MB ";
              csvSnapshot += "IOWriteTotal:" + std::to_string(static_cast<int>(sample.ioWriteMB * 10) / 10.0) + "MB ";
              csvSnapshot += "DiskReadLat:" + std::to_string(static_cast<int>(sample.diskReadLatencyMs * 10) / 10.0) + "ms ";
              csvSnapshot += "DiskWriteLat:" + std::to_string(static_cast<int>(sample.diskWriteLatencyMs * 10) / 10.0) + "ms ";
              csvSnapshot += "DiskQ:" + std::to_string(static_cast<int>(sample.diskQueueLength * 10) / 10.0) + " | ";
              
              // Kernel metrics (from CPUKernelTracker)
              csvSnapshot += "CtxSw:" + std::to_string(static_cast<int>(sample.contextSwitchesPerSec)) + "/s ";
              csvSnapshot += "Interrupts:" + std::to_string(static_cast<int>(sample.interruptsPerSec)) + "/s ";
              csvSnapshot += "DPCs:" + std::to_string(static_cast<int>(sample.dpcCountPerSec)) + "/s ";
              csvSnapshot += "DPCLat:" + std::to_string(static_cast<int>(sample.avgDpcLatencyUs * 10) / 10.0) + "μs ";
              csvSnapshot += "DPC>50μs:" + std::to_string(static_cast<int>(sample.dpcLatenciesAbove50us * 10) / 10.0) + "% ";
              csvSnapshot += "DPC>100μs:" + std::to_string(static_cast<int>(sample.dpcLatenciesAbove100us * 10) / 10.0) + "% | ";
              
              // Power state metrics (from PDH)
              csvSnapshot += "C1Time:" + std::to_string(static_cast<int>(pdhCache.cpuC1Time * 10) / 10.0) + "% ";
              csvSnapshot += "C2Time:" + std::to_string(static_cast<int>(pdhCache.cpuC2Time * 10) / 10.0) + "% ";
              csvSnapshot += "C3Time:" + std::to_string(static_cast<int>(pdhCache.cpuC3Time * 10) / 10.0) + "% ";
              csvSnapshot += "C1Trans:" + std::to_string(static_cast<int>(pdhCache.cpuC1TransitionsPerSec)) + "/s ";
              csvSnapshot += "C2Trans:" + std::to_string(static_cast<int>(pdhCache.cpuC2TransitionsPerSec)) + "/s ";
              csvSnapshot += "C3Trans:" + std::to_string(static_cast<int>(sample.cpuC3TransitionsPerSec)) + "/s | ";
              
              // Additional PDH metrics
              csvSnapshot += "DPCRate:" + std::to_string(static_cast<int>(pdhCache.cpuDpcRate)) + " ";
              csvSnapshot += "IntTime:" + std::to_string(static_cast<int>(pdhCache.cpuInterruptTime * 10) / 10.0) + "% ";
              csvSnapshot += "DPCTime:" + std::to_string(static_cast<int>(pdhCache.cpuDpcTime * 10) / 10.0) + "% | ";
              
              // Metadata
              csvSnapshot += "Cores:" + std::to_string(sample.perCoreCpuUsagePdh.size()) + " ";
              csvSnapshot += "ProcCount:" + std::to_string(sample.processCount) + " ";
              csvSnapshot += "PresentCount:" + std::to_string(sample.presentCount) + " ";
              csvSnapshot += "Timestamp:" + std::to_string(sample.timestamp);
              
              BenchmarkLogger::logStatus(csvSnapshot);
              lastCsvSnapshot = now;
              firstSnapshot = false;
            }
            
            if (batchBuffer.size() >= BATCH_SIZE_SECONDS ||
                i == durationSeconds || m_shouldStop) {
              if (saveToFile) {
                //LogCritical("Writing batch of " + std::to_string(batchBuffer.size()) + " data points to CSV");
                
                // Initialize file if needed
                if (m_firstWriteNeeded) {
                  m_resultFileManager->initializeOutputFile(m_outputFilename);
                  m_resultFileManager->writeHeader();
                  m_firstWriteNeeded = false;
                }
                
                // Find disk names for this batch
                std::set<std::string> diskNames;
                for (const auto& data : batchBuffer) {
                  for (const auto& [diskName, _] : data.perDiskReadRates) {
                    diskNames.insert(diskName);
                  }
                }
                
                m_resultFileManager->writeDataPoints(batchBuffer, diskNames);
              } else {
                LogCritical("Skipping CSV write - saveToFile is false");
              }
              batchBuffer.clear();
            }
          } else {
            if (BenchmarkLogger::shouldLogSample()) {
              LogCritical("Not collecting data - benchmark state is not RUNNING");
            }
          }
          
          // *** SIMPLIFIED: Break out of loop when COOLDOWN is detected ***
          if (currentBenchmarkState == BenchmarkStateTracker::State::COOLDOWN) {
            BenchmarkLogger::logStatus("Benchmark complete - processing final data");
            
            // Write any remaining batch data
            if (!batchBuffer.empty() && saveToFile) {
              try {
                // Initialize file if needed
                if (m_firstWriteNeeded) {
                  m_resultFileManager->initializeOutputFile(m_outputFilename);
                  m_resultFileManager->writeHeader();
                  m_firstWriteNeeded = false;
                }
                
                // Find disk names for this batch
                std::set<std::string> diskNames;
                for (const auto& data : batchBuffer) {
                  for (const auto& [diskName, _] : data.perDiskReadRates) {
                    diskNames.insert(diskName);
                  }
                }
                
                m_resultFileManager->writeDataPoints(batchBuffer, diskNames);
              } catch (const std::exception& e) {
                LogError("Exception during final batch write: " + std::string(e.what()));
              }
              batchBuffer.clear();
            }
            
            // Signal the loop to exit
            break;
          }
          
          // NEW: Emit UI metrics every 1 second from main loop for consistent timing
          // UI updates now use coherent data from caches - no delay needed
          
          try {
            // For UI display: Replace CSV per-second percentiles with cumulative percentiles
            // This ensures onBenchmarkSample() receives the correct cumulative data
            if (currentBenchmarkState == BenchmarkStateTracker::State::RUNNING) {
              // Calculate cumulative percentiles for UI display
              calculateCumulativeFrameTimePercentiles();
              
              if (m_cumulativeFrameTime1pct > 0) {
                sample.lowFps1Percent = 1000.0f / m_cumulativeFrameTime1pct;
              } else {
                sample.lowFps1Percent = 0.0f;
              }
              
              if (m_cumulativeFrameTime5pct > 0) {
                sample.lowFps5Percent = 1000.0f / m_cumulativeFrameTime5pct;
              } else {
                sample.lowFps5Percent = 0.0f;
              }
              
              if (m_cumulativeFrameTime05pct > 0) {
                sample.lowFps05Percent = 1000.0f / m_cumulativeFrameTime05pct;
              } else {
                sample.lowFps05Percent = 0.0f;
              }
            } else {
              // During non-RUNNING states: Show 0 since no benchmark data exists
              sample.lowFps1Percent = 0.0f;
              sample.lowFps5Percent = 0.0f;  
              sample.lowFps05Percent = 0.0f;
            }
            
            // Emit sample with cumulative percentiles to UI
            emit benchmarkSample(sample);
            
            // Keep emitUIMetrics() to ensure currentData is updated with cumulative values
            // for any legacy code that calls getLatestDataPoint()
            emitUIMetrics();
          } catch (const std::exception& e) {
            LogError("Exception emitting UI metrics: " + std::string(e.what()));
          }
        }
      }

      
      try {
        PM_StopMonitoring(processId);
      } catch (const std::exception& e) {
        LogError("Exception in PM_StopMonitoring: " + std::string(e.what()));
      }
      
      try {
        cleanup();
      } catch (const std::exception& e) {
        LogError("Exception in cleanup: " + std::string(e.what()));
      }
      
      // Emit completion signal if safe
      if (instance == this && !m_cleanupDone.load() && !m_stopBenchmarkCalled.load()) {
        try {
          emit benchmarkFinished();
          LogCritical("Benchmark completed successfully");
          // Perform automatic upload if enabled
          performAutomaticUpload();
        } catch (const std::exception& e) {
          LogError("Exception emitting benchmarkFinished: " + std::string(e.what()));
        }
      }

    } catch (const std::exception& e) {
      LogError("Exception in benchmark thread: " + std::string(e.what()));
      cleanup();
      emit benchmarkError("Benchmark failed: " +
                         QString::fromStdString(e.what()));
    }
    
    // Set safety flags before thread completion
    if (instance) {
      instance->m_benchmarkEndDetected.store(true, std::memory_order_release);
      instance->m_cleanupDone.store(true, std::memory_order_release);
      instance->m_stopBenchmarkCalled.store(true, std::memory_order_release);
      std::atomic_thread_fence(std::memory_order_seq_cst);
    }
  });

  // Detach thread to prevent destruction issues
  try {
    benchmarkThread.detach();
  } catch (const std::exception& e) {
    LogError("Exception detaching thread: " + std::string(e.what()));
  }

  LogCritical("Benchmark started - collecting metrics");
  
  return true;
}

void BenchmarkManager::cleanup() {
  
  // Stop RustLogMonitor to prevent continued monitoring
  if (m_stateTracker) {
    m_stateTracker->cleanup();
  }
  
  if (m_currentProcessId != 0) {
    try {
      stopBenchmark();
    } catch (const std::exception& e) {
      LogError("Exception in stopBenchmark during cleanup: " + std::string(e.what()));
    }
  } else {
    std::lock_guard<std::mutex> lock(g_sessionMutex);
    if (!g_activeSessions.empty()) {
      for (const auto& session : g_activeSessions) {
        try {
          KillExistingEtwSession(session);
        } catch (const std::exception& e) {
          LogError("Exception killing ETW session: " + std::string(e.what()));
        }
      }
      g_activeSessions.clear();
    }
  }
}

// emitMetrics removed - dead code, replaced by benchmarkSample signal

// NEW: Dedicated UI metrics emission for consistent 1-second updates
void BenchmarkManager::emitUIMetrics() {
  // Only emit UI updates during RUNNING or WAITING state (allow GPU data display during waiting)
  if (m_benchmarkEndDetected.load() || m_cleanupDone.load() || m_stopBenchmarkCalled.load() ||
      currentBenchmarkState == BenchmarkStateTracker::State::COOLDOWN ||
      currentBenchmarkState == BenchmarkStateTracker::State::OFF) {
    return;
  }

  PM_METRICS uiMetrics;
  bool hasValidData = false;
  
  // Use the already-computed sample from main loop instead of rebuilding from caches
  BenchmarkDataPoint sample;
  {
    std::lock_guard<std::recursive_mutex> lock(dataMutex);
    sample = lastCommittedSample;  // Already has cumulative percentiles calculated by main loop
  }
  
  {
    std::lock_guard<std::recursive_mutex> lock(dataMutex);
    
    // During RUNNING: Only emit if we have meaningful FPS data (indicates ETW is working)
    // During WAITING: Always emit to show GPU/system data even without FPS
    bool isRunning = (currentBenchmarkState == BenchmarkStateTracker::State::RUNNING);
    bool hasFpsData = (sample.fps > 0 && sample.presentCount > 0);
    
    if (isRunning ? hasFpsData : true) {
      // Build complete UI metrics from sample with all data synchronized
      uiMetrics.fps = sample.fps;
      uiMetrics.frametime = sample.frameTime;
      uiMetrics.cpuRenderTime = sample.cpuRenderTime;
      uiMetrics.gpuRenderTime = sample.gpuRenderTime;
  uiMetrics.maxFrameTime = sample.highestFrameTime;
  // Also provide max CPU/GPU render times for UI consumers
  uiMetrics.maxGpuRenderTime = sample.highestGpuTime;
  uiMetrics.maxCpuRenderTime = sample.highestCpuTime;
      uiMetrics.destWidth = sample.destWidth;
      uiMetrics.destHeight = sample.destHeight;
      
      // Use the already-calculated cumulative values from main loop
      if (sample.lowFps1Percent > 0) {
        uiMetrics.frameTime99Percentile = 1000.0f / sample.lowFps1Percent;
      } else {
        uiMetrics.frameTime99Percentile = 0.0f; // No data or WAITING state
      }
      
      if (sample.lowFps5Percent > 0) {
        uiMetrics.frameTime95Percentile = 1000.0f / sample.lowFps5Percent;
      } else {
        uiMetrics.frameTime95Percentile = 0.0f; // No data or WAITING state
      }
      
      if (sample.lowFps05Percent > 0) {
        uiMetrics.frameTime995Percentile = 1000.0f / sample.lowFps05Percent;
      } else {
        uiMetrics.frameTime995Percentile = 0.0f; // No data or WAITING state
      }
      
      hasValidData = true;
      
      if (BenchmarkLogger::shouldLogStatus()) {
        BenchmarkLogger::logStatus(
          std::string("[UI-1s] State: ") + (isRunning ? "RUN" : "WAIT") + 
          " FPS: " + std::to_string(static_cast<int>(uiMetrics.fps)) + 
          " CPU: " + std::to_string(static_cast<int>(sample.procProcessorTime)) + "% " +
          "GPU: " + std::to_string(static_cast<int>(sample.gpuUtilization)) + "% " +
          "VRAM: " + std::to_string(static_cast<int>(sample.gpuMemUsed / (1024*1024*1024))) + "GB " +
          "Temp: " + std::to_string(static_cast<int>(sample.gpuTemp)) + "C");
      }
    }
  }
  
  // Only emit if we have complete, valid data
  if (hasValidData) {
    try {
      emit benchmarkMetrics(uiMetrics);
    } catch (const std::exception& e) {
      LogError("Exception emitting UI metrics: " + std::string(e.what()));
    }
  }
  
  // Update currentData for legacy compatibility with getLatestDataPoint()
  {
    std::lock_guard<std::recursive_mutex> lock(dataMutex);
    currentData = sample;
  }
}


// Calculate cumulative percentiles for UI display (only from RUNNING state data)
void BenchmarkManager::calculateCumulativeFrameTimePercentiles() {
  std::lock_guard<std::mutex> lock(frameTimesMutex);

  // DEBUG: Always log the current state and data status each second
  static auto lastDebugLog = std::chrono::steady_clock::now();
  auto now = std::chrono::steady_clock::now();
  

  // CRITICAL: Only calculate if we're in RUNNING state and have accumulated data
  if (currentBenchmarkState != BenchmarkStateTracker::State::RUNNING || frameTimeHistogram.totalSamples == 0) {
    // Initialize to invalid values if not in RUNNING state or no data
    m_cumulativeFrameTime1pct = -1.0f;
    m_cumulativeFrameTime5pct = -1.0f;
    m_cumulativeFrameTime05pct = -1.0f;
    
    return;
  }

  // Additional safety check: only count frame times from when the benchmark actually started
  if (m_benchmarkStartTime == std::chrono::steady_clock::time_point{}) {
    // Benchmark hasn't started yet, so no cumulative data
    m_cumulativeFrameTime1pct = -1.0f;
    m_cumulativeFrameTime5pct = -1.0f;
    m_cumulativeFrameTime05pct = -1.0f;
    
    return;
  }

  // Use histogram to calculate percentiles efficiently - O(buckets) instead of O(n log n)
  m_cumulativeFrameTime1pct = frameTimeHistogram.calculatePercentile(BenchmarkConstants::FPS_PERCENTILE_1);
  m_cumulativeFrameTime5pct = frameTimeHistogram.calculatePercentile(BenchmarkConstants::FPS_PERCENTILE_5);
  m_cumulativeFrameTime05pct = frameTimeHistogram.calculatePercentile(BenchmarkConstants::FPS_PERCENTILE_01);
  
  // DEBUG: Log histogram distribution and calculated percentiles each second 
  if (std::chrono::duration_cast<std::chrono::seconds>(now - lastDebugLog).count() >= 1) {
    float cumulative1pctFps = (m_cumulativeFrameTime1pct > 0) ? 1000.0f / m_cumulativeFrameTime1pct : 0.0f;
    float cumulative5pctFps = (m_cumulativeFrameTime5pct > 0) ? 1000.0f / m_cumulativeFrameTime5pct : 0.0f;
    float cumulative05pctFps = (m_cumulativeFrameTime05pct > 0) ? 1000.0f / m_cumulativeFrameTime05pct : 0.0f;
    
    // Log histogram distribution for debugging
    LOG_DEBUG << "[HISTOGRAM-DEBUG] Total samples: " << frameTimeHistogram.totalSamples 
              << ", Underflow: " << frameTimeHistogram.underflowCount 
              << ", Overflow: " << frameTimeHistogram.overflowCount;
    
    LOG_DEBUG << "[HISTOGRAM-DEBUG] Percentiles - 0.1%: " << m_cumulativeFrameTime05pct << "ms (" 
              << cumulative05pctFps << " FPS), 1%: " << m_cumulativeFrameTime1pct << "ms (" 
              << cumulative1pctFps << " FPS), 5%: " << m_cumulativeFrameTime5pct << "ms (" 
              << cumulative5pctFps << " FPS)";
    
    // Show distribution of frame times in key buckets
    LOG_DEBUG << "[HISTOGRAM-DEBUG] Sample distribution - ";
    const auto& hist = frameTimeHistogram;
    for (size_t i = 0; i < hist.BUCKET_COUNT; i += 20) { // Sample every 10ms
      float frameTime = hist.MIN_FRAME_TIME + (i * hist.BUCKET_SIZE);
      if (hist.buckets[i] > 0) {
        LOG_DEBUG << frameTime << "ms:" << hist.buckets[i] << " ";
      }
    }
    // Line ending handled by logger
    
    lastDebugLog = now;
  }
}

bool BenchmarkManager::cleanupExistingETWSessions() {
  bool success = true;

  HKEY hKey;
  if (RegOpenKeyExW(HKEY_LOCAL_MACHINE,
                    L"SYSTEM\\CurrentControlSet\\Control\\WMI\\Autologger", 0,
                    KEY_READ | KEY_WRITE, &hKey) == ERROR_SUCCESS) {

    wchar_t keyName[MAX_PATH];
    DWORD index = 0;
    DWORD nameSize = MAX_PATH;

    std::vector<std::wstring> keysToDelete;
    while (RegEnumKeyExW(hKey, index++, keyName, &nameSize, NULL, NULL, NULL,
                         NULL) == ERROR_SUCCESS) {
      if (IsCheckmarkPresentMonSessionKeyName(keyName)) {
        keysToDelete.push_back(keyName);
      }
      nameSize = MAX_PATH;
    }

    for (const auto& key : keysToDelete) {
      if (!KillExistingEtwSession(key)) {
        LogError("Failed to stop ETW session: " +
                 std::string(key.begin(), key.end()));
        success = false;
      }
      LSTATUS status = RegDeleteKeyW(hKey, key.c_str());
      if (status != ERROR_SUCCESS) {
        LogError("Failed to delete registry key: " +
                 std::string(key.begin(), key.end()) +
                 " (Status: " + std::to_string(status) + ")");
        success = false;
      }
    }

    RegCloseKey(hKey);
  }

  Sleep(500);
  return success;
}

bool BenchmarkManager::stopBenchmark() {
  LogCritical("[CLEANUP] Benchmark cleanup starting");
  
  if (m_currentProcessId == 0) {
    LogCritical("[CLEANUP] No process ID - cleanup aborted");
    return false;
  }
  
  if (m_stopBenchmarkCalled.exchange(true)) {
    LogCritical("[CLEANUP] stopBenchmark already called - ignoring");
    return true;
  }

  // Ensure NVENC warning banner is cleared when stopping the benchmark
  if (m_nvencUsageActive.exchange(false)) {
    emit nvencUsageDetected(false);
  }

  uint32_t processId = m_currentProcessId;

  // CRITICAL: Immediately set the benchmark end flag to stop all ETW callbacks
  m_benchmarkEndDetected.store(true, std::memory_order_release);
  
  // Record end time if benchmark was running
  if (currentBenchmarkState == BenchmarkStateTracker::State::RUNNING) {
    m_benchmarkEndTime = std::chrono::steady_clock::now();
  }
  
  // Force memory barrier to ensure ETW callbacks see the flag changes immediately
  std::atomic_thread_fence(std::memory_order_seq_cst);
  

  m_shouldStop = true;
  m_currentProcessId = 0;

  if (currentBenchmarkState == BenchmarkStateTracker::State::RUNNING &&
      m_stateTracker) {
    m_stateTracker->stopBenchmark();
  }

  currentBenchmarkState = BenchmarkStateTracker::State::OFF;
  emit benchmarkStateChanged(
    "<font color='#FFFFFF'>Benchmark: </font><font color='#FFFFFF'>OFF</font>");

  try {
    auto status = PM_StopMonitoring(processId);

    {
      std::lock_guard<std::mutex> lock(g_sessionMutex);
      std::wstring sessionName =
        L"PresentMon_Session_" + std::to_wstring(processId);

      KillExistingEtwSession(sessionName);
      g_activeSessions.erase(sessionName);
    }

    LogCritical("Final write check - Valid benchmark: " + 
                std::to_string(m_stateTracker ? m_stateTracker->isValidBenchmark() : false) + 
                ", Final write done: " + std::to_string(m_finalWriteDone) + 
                ", allData.size(): " + std::to_string(allData.size()));
                
    if (m_stateTracker && m_stateTracker->isValidBenchmark() &&
        !m_finalWriteDone) {
      LogCritical("Triggering final CSV write with " + std::to_string(allData.size()) + " total data points");
      bool originalSaveToFile = saveToFile;
      saveToFile = true;

      // Use the new file manager for final benchmark processing
      std::vector<BenchmarkDataPoint> allDataCopy;
      {
        std::lock_guard<std::recursive_mutex> lock(dataMutex);
        allDataCopy = allData;
      }
      
      m_resultFileManager->finalizeBenchmark(allDataCopy, m_userSystemId);

      saveToFile = originalSaveToFile;
      LogCritical("Final CSV write completed");
    } else {
      LogCritical("Skipping final CSV write - benchmark invalid or already written");
    }

    m_finalWriteDone = true;

    // Cleanup PDH interface with synchronization
    LogCritical("[CLEANUP] Starting PDH cleanup");
    if (m_pdhInterface && !m_cleanupDone.exchange(true)) {
      try {
        LogCritical("[CLEANUP] Stopping PDH interface");
        m_pdhInterface->stop();
        LogCritical("[CLEANUP] Resetting PDH interface");
        m_pdhInterface.reset();
        LogCritical("[CLEANUP] PDH cleanup completed successfully");
      } catch (const std::exception& e) {
        LogError("[CLEANUP] Exception during PDH cleanup: " + std::string(e.what()));
      }
    } else {
      LogCritical("[CLEANUP] PDH interface already cleaned or null");
    }

    // Stop metrics providers
    LogCritical("[CLEANUP] Starting GPU metrics cleanup");
    if (m_gpuMetrics) {
      try {
        LogCritical("[CLEANUP] Stopping GPU metrics collection");
        m_gpuMetrics->stopCollecting();
        LogCritical("[CLEANUP] Resetting GPU metrics");
        m_gpuMetrics.reset();
        LogCritical("[CLEANUP] GPU metrics cleanup completed");
      } catch (const std::exception& e) {
        LogError("[CLEANUP] Exception stopping GPU metrics: " + std::string(e.what()));
        m_gpuMetrics.reset();
      }
    } else {
      LogCritical("[CLEANUP] GPU metrics already null");
    }

    if (m_cpuKernelTracker) {
      bool trackerStopped = false;
      for (int attempt = 0; attempt < 2; attempt++) {
        std::thread stopThread([this, &trackerStopped]() {
          try {
            m_cpuKernelTracker->stopTracking();
            trackerStopped = true;
          } catch (const std::exception& e) {
            LOG_ERROR << "[ERROR] Exception stopping CPU tracker: " << e.what();
          }
        });
        stopThread.detach();

        for (int i = 0; i < 30 && !trackerStopped; i++) {
          std::this_thread::sleep_for(std::chrono::milliseconds(100));
        }
        if (trackerStopped) break;
      }

      // Wait for ETW callbacks to complete
      std::this_thread::sleep_for(std::chrono::milliseconds(300));
      m_cpuKernelTracker.reset();
      std::this_thread::sleep_for(std::chrono::milliseconds(200));
    }

    if (m_diskTracker) {
      try {
        m_diskTracker->stopTracking();
        std::this_thread::sleep_for(std::chrono::milliseconds(400));
        m_diskTracker.reset();
        std::this_thread::sleep_for(std::chrono::milliseconds(300));
      } catch (const std::exception& e) {
        LOG_ERROR << "[ERROR] Exception stopping disk tracker: " << e.what();
        std::this_thread::sleep_for(std::chrono::milliseconds(400));
        m_diskTracker.reset();
      }
    }

    if (benchmarkThread.joinable()) {
      std::condition_variable cv;
      std::mutex mtx;
      bool threadDone = false;

      std::thread joinThread([this, &cv, &mtx, &threadDone]() {
        if (benchmarkThread.joinable()) {
          benchmarkThread.join();
          {
            std::lock_guard<std::mutex> lock(mtx);
            threadDone = true;
          }
          cv.notify_one();
        }
      });

      {
        std::unique_lock<std::mutex> lock(mtx);
        if (!cv.wait_for(lock, std::chrono::seconds(3),
                         [&threadDone] { return threadDone; })) {
          joinThread.detach();
        } else {
          joinThread.join();
        }
      }
    }

    cleanupRegistryEntries();
    PM_Shutdown();

    {
      std::lock_guard<std::mutex> lock(g_sessionMutex);
      if (!g_activeSessions.empty()) {
        for (const auto& session : g_activeSessions) {
          KillExistingEtwSession(session);
        }
        g_activeSessions.clear();
      }
    }

    if (m_stateTracker && m_stateTracker->isValidBenchmark()) {
      QString specsPath =
        QString::fromStdString("benchmark_results/") + m_outputFilename;
      specsPath.replace(".csv", "_specs.txt");
      BenchmarkSpecsFileManager::updateSpecsFileStatus(specsPath, false);

      // Export current optimization settings to JSON using the new
      // ExportSettings system
      QString optimizationSettingsFile =
        QString("benchmark_results/optimization_settings_%1.json")
          .arg(m_benchmarkHash);

      // Export current settings to JSON (not backup values, but actual current
      // system state)
      auto exportResult = optimizations::ExportSettings::ExportAllSettings(
        optimizationSettingsFile.toStdString(),
        true  // include metadata
      );

      if (exportResult.success) {
        LOG_INFO << "Optimization settings exported to: [file path hidden for privacy] ("
                  << exportResult.exported_settings << "/"
                  << exportResult.total_settings << " settings)";
      } else {
        LogError("Failed to export optimization settings: " +
                 exportResult.error_message);
      }
    }

    LogCritical("[CLEANUP] Emitting benchmarkFinished signal");
    emit benchmarkFinished();
    // Perform automatic upload if enabled
    performAutomaticUpload();
    
    return true;

  } catch (const std::exception& e) {
    LogError("Exception during stopBenchmark: " + std::string(e.what()));

    try {
      if (m_cpuKernelTracker) {
        m_cpuKernelTracker->stopTracking();
        m_cpuKernelTracker.reset();
      }

      if (m_diskTracker) {
        m_diskTracker->stopTracking();
        m_diskTracker.reset();
      }

      if (m_gpuMetrics) {
        m_gpuMetrics->stopCollecting();
        m_gpuMetrics.reset();
      }
    } catch (...) {
    }

    return false;
  }
}

void BenchmarkManager::accumulateMetrics(const PM_METRICS& metrics) {
  // CRITICAL: Only accumulate frame time data when benchmark is actually RUNNING
  // This ensures cumulative metrics only include frames from the actual benchmark period
  if (currentBenchmarkState != BenchmarkStateTracker::State::RUNNING) {
    return; // Don't accumulate during WAITING, COOLDOWN, or OFF states
  }
  
  if (metrics.fps <= 0 || metrics.frameCount <= 0) {
    return;
  }
  
  // Calculate timestamp once outside locks
  auto currentTime = std::chrono::steady_clock::now();
  float timestamp = std::chrono::duration_cast<std::chrono::milliseconds>(
                      currentTime - benchmarkStartTime)
                      .count() / 1000.0f;

  // Gate accumulation to ~1 Hz to avoid double-counting the sliding 1s window
  static std::chrono::steady_clock::time_point lastAccumulation = std::chrono::steady_clock::now();
  auto elapsed = std::chrono::duration_cast<std::chrono::milliseconds>(currentTime - lastAccumulation).count();
  
  if (elapsed < 900) { // Only accumulate approximately every 900ms to avoid overlapping 1s windows
    return;
  }
  lastAccumulation = currentTime;

  // Create single FPS and frame time entries using per-second aggregated values (same as CSV export)
  float avgFps = metrics.fps;
  float avgFrameTime = metrics.frametime;
  
  // Add single representative sample points for this 1-second aggregation period
  // These represent the same "final" values that will appear in CSV
  {
    std::lock_guard<std::mutex> fpsLock(fpsValuesMutex);
    allFpsSamples.emplace_back(FpsDataPoint{avgFps, timestamp});
  }
  
  {
    std::lock_guard<std::mutex> frameTimesLock(frameTimesMutex);
    allFrameTimePoints.emplace_back(FrameTimePoint{avgFps, avgFrameTime, timestamp});
    
    // Feed histogram directly using the per-second stats distribution
    // This maintains the same statistical profile as the old synthetic method
    if (metrics.frameTime99Percentile > 0 && metrics.frameTime95Percentile > 0 && metrics.frameTime995Percentile > 0) {
      int totalFrames = metrics.frameCount;
      
      // Calculate frame counts for each percentile bucket
      int frames995 = std::max(1, (int)(totalFrames * 0.005f)); // 0.5% worst frames
      int frames99 = std::max(1, (int)(totalFrames * 0.01f));   // 1% worst frames  
      int frames95 = std::max(1, (int)(totalFrames * 0.05f));   // 5% worst frames
      int framesNormal = totalFrames - frames995 - frames99 - frames95; // Rest
      
      // Add to histogram in batches - much more efficient than per-frame loops
      frameTimeHistogram.addFrameTime(metrics.frameTime995Percentile, frames995);
      frameTimeHistogram.addFrameTime(metrics.frameTime99Percentile, frames99);
      frameTimeHistogram.addFrameTime(metrics.frameTime95Percentile, frames95);
      frameTimeHistogram.addFrameTime(metrics.frametime, framesNormal);
    } else {
      // Fallback: if percentiles are not available, use average frametime
      frameTimeHistogram.addFrameTime(metrics.frametime, metrics.frameCount);
    }
  }
}

void BenchmarkManager::setSaveToFile(bool save) { saveToFile = save; }

// Simple signal handlers that replace complex state logic
void BenchmarkManager::handleBenchmarkStart() {
  LogCritical("handleBenchmarkStart() called");
  
  // Set state and notify UI
  BenchmarkStateTracker::State oldState = currentBenchmarkState;
  currentBenchmarkState = BenchmarkStateTracker::State::RUNNING;
  
  // Record start time for data collection
  m_benchmarkStartTime = std::chrono::steady_clock::now();
  
  // Emit state change signal to update UI
  if (oldState != BenchmarkStateTracker::State::RUNNING) {
    QString stateStr = "<font color='#FFFFFF'>Benchmark: </font>";
    stateStr += "<font color='#00FF00'>Running...</font>";
    LogCritical("State: RUNNING - emitting benchmarkStateChanged signal");
    emit benchmarkStateChanged(stateStr);
  }
}

void BenchmarkManager::handleBenchmarkEnd() {
  m_benchmarkEndDetected.store(true, std::memory_order_release);
  BenchmarkStateTracker::State oldState = currentBenchmarkState;
  currentBenchmarkState = BenchmarkStateTracker::State::COOLDOWN;
  m_benchmarkEndTime = std::chrono::steady_clock::now();
  std::atomic_thread_fence(std::memory_order_seq_cst);
  
  // Emit state change signal to update UI
  if (oldState != BenchmarkStateTracker::State::COOLDOWN) {
    QString stateStr = "<font color='#FFFFFF'>Benchmark: </font>";
    stateStr += "<font color='#FF9900'>Finalizing...</font>";
    LogCritical("State: COOLDOWN");
    emit benchmarkStateChanged(stateStr);
  }
}

void BenchmarkManager::updateBenchmarkState(const PM_METRICS& metrics) {
  if (m_benchmarkEndDetected.load()) {
    return;
  }
  
  if (!m_stateTracker) {
    return;
  }

  BenchmarkDataPoint processMetrics;
  {
    std::lock_guard<std::recursive_mutex> lock(dataMutex);
    processMetrics = currentData;
  }

  BenchmarkStateTracker::State newState;
  try {
    newState = m_stateTracker->updateState(metrics, processMetrics);
  } catch (const std::exception& e) {
    LogError("Exception in updateState: " + std::string(e.what()));
    return;
  } catch (...) {
    LogError("Unknown exception in updateState");
    return;
  }

  if (newState == BenchmarkStateTracker::State::COOLDOWN &&
      currentBenchmarkState != BenchmarkStateTracker::State::COOLDOWN) {

    currentBenchmarkState = BenchmarkStateTracker::State::COOLDOWN;

    try {
      auto [startTime, endTime] = m_stateTracker->getBenchmarkTimeRange();
      LogCritical("Benchmark range: " + std::to_string(static_cast<int>(startTime)) +
                 "s to " + std::to_string(static_cast<int>(endTime)) + "s");
    } catch (const std::exception& e) {
      LogError("Exception in getBenchmarkTimeRange: " + std::string(e.what()));
    }


    return;
  }

  if (newState != currentBenchmarkState) {
    QString stateStr = "<font color='#FFFFFF'>Benchmark: </font>";
    std::string logState;

    switch (newState) {
      case BenchmarkStateTracker::State::OFF:
        stateStr += "<font color='#FFFFFF'>OFF</font>";
        logState = "OFF";
        break;

      case BenchmarkStateTracker::State::WAITING:
        stateStr += "<font color='#FFD700'>Waiting...</font>";
        logState = "WAITING";
        break;

      case BenchmarkStateTracker::State::RUNNING:
        stateStr += "<font color='#00FF00'>Running...</font>";
        logState = "RUNNING";
        break;

      case BenchmarkStateTracker::State::COOLDOWN:
        stateStr += "<font color='#FF9900'>Finalizing...</font>";
        logState = "COOLDOWN";
        break;
    }

    if (logState == "RUNNING" || logState == "COOLDOWN") {
      LogCritical("State: " + logState);
    }
    emit benchmarkStateChanged(stateStr);
    currentBenchmarkState = newState;
  }
}

std::chrono::steady_clock::time_point BenchmarkManager::getBenchmarkStartTime()
  const {
  if (!m_stateTracker) return {};

  const auto& transitions = m_stateTracker->getTransitions();
  auto it = std::find_if(
    transitions.begin(), transitions.end(),
    [](const BenchmarkStateTracker::StateTransition& t) { return t.isStart; });

  return it != transitions.end() ? it->timestamp
                                 : std::chrono::steady_clock::time_point{};
}

std::chrono::steady_clock::time_point BenchmarkManager::getBenchmarkEndTime()
  const {
  if (!m_stateTracker) return {};

  const auto& transitions = m_stateTracker->getTransitions();
  auto it = std::find_if(
    transitions.rbegin(), transitions.rend(),
    [](const BenchmarkStateTracker::StateTransition& t) { return !t.isStart; });

  return it != transitions.rend() ? it->timestamp
                                  : std::chrono::steady_clock::time_point{};
}

void BenchmarkManager::copyRustBenchmarkFiles() {
  static std::atomic<bool> copyInProgress{false};

  if (copyInProgress.exchange(true)) {
    return;
  }

  try {
    std::this_thread::sleep_for(std::chrono::seconds(3));

    QDateTime currentTime = QDateTime::currentDateTime();
    QString latestFile = RustBenchmarkFinder::findLatestBenchmark();

    if (latestFile.isEmpty()) {
      copyInProgress = false;
      return;
    }

    QFileInfo fi(latestFile);
    QDateTime fileTime = fi.lastModified();

    int secondsOld = fileTime.secsTo(currentTime);
    if (secondsOld > 300) {
      copyInProgress = false;
      return;
    }

    if (m_benchmarkHash.isEmpty()) {
      LogError("No benchmark hash available for naming Rust benchmark file");
      copyInProgress = false;
      return;
    }

    QString newName =
      QString("%1_%2%3")
        .arg(fi.baseName())
        .arg(m_benchmarkHash)
        .arg(fi.completeSuffix().isEmpty() ? "" : "." + fi.completeSuffix());

    QString destPath = "benchmark_results/" + newName;

    // Read the JSON file, add our user system ID and then save it
    QFile jsonFile(latestFile);
    if (jsonFile.open(QIODevice::ReadOnly)) {
      QJsonDocument doc = QJsonDocument::fromJson(jsonFile.readAll());
      jsonFile.close();

      if (!doc.isNull() && doc.isObject()) {
        QJsonObject obj = doc.object();

        // Add UserSystemID to the JSON
        obj["userSystemId"] = m_userSystemId;

        // Add benchmark hash
        obj["benchmarkHash"] = m_benchmarkHash;

        // Update the document
        doc.setObject(obj);

        // Create destination file
        QFile outFile(destPath);
        if (outFile.open(QIODevice::WriteOnly)) {
          outFile.write(doc.toJson());
          outFile.close();
        } else {
          // Fallback to simple copy if we can't modify the JSON
          QFile::copy(latestFile, destPath);
        }
      } else {
        // Not valid JSON, just copy the file
        QFile::copy(latestFile, destPath);
      }
    } else {
      // Couldn't open the file, just copy it
      QFile::copy(latestFile, destPath);
    }

    copyInProgress = false;
  } catch (...) {
    copyInProgress = false;
  }
}

float calculatePercentile(const std::vector<float>& sortedValues,
                          float percentile) {
  if (sortedValues.empty()) {
    return 0.0f;
  }

  if (sortedValues.size() == 1) {
    return sortedValues[0];
  }

  float fraction = percentile / 100.0f;
  size_t index = static_cast<size_t>(sortedValues.size() * fraction);
  index = std::min(index, sortedValues.size() - 1);

  return sortedValues[index];
}

// Add new method for PDH metrics accumulation
void BenchmarkManager::accumulatePdhMetrics() {
  if (!m_pdhInterface || !m_pdhInterface->isRunning()) {
    static RateLimitedLog warningLog(10000); // Log warning only every 10 seconds
    warningLog.log("[PDH] WARNING: PDH interface not available - metrics will show -1");
    
    // Set all PDH-dependent metrics to -1 to indicate missing data
    std::lock_guard<std::mutex> lock(pdhMutex);
    
    // CPU metrics
    pdhCache.procProcessorTime = -1;
    pdhCache.procUserTime = -1;
    pdhCache.procPrivilegedTime = -1;
    pdhCache.procIdleTime = -1;
    pdhCache.procActualFreq = -1;
    pdhCache.cpuInterruptsPerSec = -1;
    pdhCache.cpuDpcTime = -1;
    pdhCache.cpuInterruptTime = -1;
    pdhCache.cpuDpcsQueuedPerSec = -1;
    pdhCache.cpuDpcRate = -1;
    pdhCache.cpuC1Time = -1;
    pdhCache.cpuC2Time = -1;
    pdhCache.cpuC3Time = -1;
    pdhCache.cpuC1TransitionsPerSec = -1;
    pdhCache.cpuC2TransitionsPerSec = -1;
    pdhCache.cpuC3TransitionsPerSec = -1;
    
    // Memory metrics
    pdhCache.availableMemoryMB = -1;
    pdhCache.memoryLoad = -1;
    pdhCache.memoryCommittedBytes = -1;
    pdhCache.memoryCommitLimit = -1;
    pdhCache.memoryFaultsPerSec = -1;
    pdhCache.memoryPagesPerSec = -1;
    pdhCache.memoryPoolNonPagedBytes = -1;
    pdhCache.memoryPoolPagedBytes = -1;
    pdhCache.memorySystemCodeBytes = -1;
    pdhCache.memorySystemDriverBytes = -1;
    
    // Disk metrics
    pdhCache.ioReadRateMBs = -1;
    pdhCache.ioWriteRateMBs = -1;
    pdhCache.diskReadsPerSec = -1;
    pdhCache.diskWritesPerSec = -1;
    pdhCache.diskTransfersPerSec = -1;
    pdhCache.diskBytesPerSec = -1;
    pdhCache.diskAvgReadQueueLength = -1;
    pdhCache.diskAvgWriteQueueLength = -1;
    pdhCache.diskAvgQueueLength = -1;
    pdhCache.diskAvgReadTime = -1;
    pdhCache.diskAvgWriteTime = -1;
    pdhCache.diskAvgTransferTime = -1;
    pdhCache.diskPercentTime = -1;
    pdhCache.diskPercentReadTime = -1;
    pdhCache.diskPercentWriteTime = -1;
    
    // System metrics
    pdhCache.contextSwitchesPerSec = -1;
    pdhCache.systemProcessorQueueLength = -1;
    pdhCache.systemProcesses = -1;
    pdhCache.systemThreads = -1;
    pdhCache.pdhInterruptsPerSec = -1;
    
    // Per-core metrics (clear vectors when PDH not available)
    pdhCache.perCoreCpuUsage.clear();
    pdhCache.perCoreActualFreq.clear();
    
    return;
  }

  static int callCount = 0;
  static int missingMetricsCount = 0;
  static int totalMetricsExpected = 0;
  static std::chrono::steady_clock::time_point lastFullReport;
  static bool firstCall = true;
  
  callCount++;
  
  if (firstCall) {
    totalMetricsExpected = m_pdhInterface->getAvailableMetrics().size();
    lastFullReport = std::chrono::steady_clock::now();
    firstCall = false;
    
    if (totalMetricsExpected == 0) {
      LogError("PDH metrics count is 0 - initialization may need more time");
    }
  }
  
  // Retry getting metrics count if we had 0 initially (PDH might need time to initialize)
  if (totalMetricsExpected == 0 && callCount <= 5) {
    totalMetricsExpected = m_pdhInterface->getAvailableMetrics().size();
  }
  
  // If still 0 after retries, set count based on actual essential metrics count
  if (totalMetricsExpected == 0 && callCount > 5) {
    // Based on essential metrics from PdhMetricDefinitions.h:
    // CPU: 18 metrics, Memory: 9 metrics, Disk: 18 metrics, System: 5 metrics = 50 total
    totalMetricsExpected = 50; 
  }
  
  size_t procCount = m_pdhInterface->getCpuCoreCount();
  if (procCount <= 0) {
    static RateLimitedLog coreWarningLog(10000);
    coreWarningLog.log("[PDH] WARNING: CPU core count is 0");
    return;
  }

  {
    std::lock_guard<std::mutex> lock(pdhMutex);

    int currentMissing = 0;

    // Helper function to safely get a metric value and track missing ones
    // Critical: Always return -1 for missing data, never make up backup values
    auto getMetricValue = [&currentMissing](PdhInterface* pdhInterface, const std::string& metricName) -> double {
      double value = -1.0;
      bool success = pdhInterface->getMetric(metricName, value);
      if (!success || value < 0) {
        currentMissing++;
        return -1.0;  // Always use -1 for invalid/missing data
      }
      return value;
    };

    // Collect essential PDH metrics with missing count tracking
    
    // CPU usage metrics
    pdhCache.procProcessorTime = getMetricValue(m_pdhInterface.get(), "cpu_total_usage");
    pdhCache.procUserTime = getMetricValue(m_pdhInterface.get(), "cpu_user_time");
    pdhCache.procPrivilegedTime = getMetricValue(m_pdhInterface.get(), "cpu_privileged_time");
    pdhCache.procIdleTime = getMetricValue(m_pdhInterface.get(), "cpu_idle_time");
    
    // CPU frequency metrics
    pdhCache.procActualFreq = getMetricValue(m_pdhInterface.get(), "cpu_actual_frequency");
    
    // CPU interrupt metrics
    pdhCache.cpuInterruptsPerSec = getMetricValue(m_pdhInterface.get(), "cpu_interrupts_per_sec");
    pdhCache.cpuDpcTime = getMetricValue(m_pdhInterface.get(), "cpu_dpc_time");
    pdhCache.cpuInterruptTime = getMetricValue(m_pdhInterface.get(), "cpu_interrupt_time");
    pdhCache.cpuDpcsQueuedPerSec = getMetricValue(m_pdhInterface.get(), "cpu_dpcs_queued_per_sec");
    pdhCache.cpuDpcRate = getMetricValue(m_pdhInterface.get(), "cpu_dpc_rate");
    
    // CPU power state metrics
    pdhCache.cpuC1Time = getMetricValue(m_pdhInterface.get(), "cpu_c1_time");
    pdhCache.cpuC2Time = getMetricValue(m_pdhInterface.get(), "cpu_c2_time");
    pdhCache.cpuC3Time = getMetricValue(m_pdhInterface.get(), "cpu_c3_time");
    pdhCache.cpuC1TransitionsPerSec = getMetricValue(m_pdhInterface.get(), "cpu_c1_transitions_per_sec");
    pdhCache.cpuC2TransitionsPerSec = getMetricValue(m_pdhInterface.get(), "cpu_c2_transitions_per_sec");
    pdhCache.cpuC3TransitionsPerSec = getMetricValue(m_pdhInterface.get(), "cpu_c3_transitions_per_sec");
    
    // Memory system metrics
    pdhCache.availableMemoryMB = getMetricValue(m_pdhInterface.get(), "memory_available_mbytes");
    pdhCache.memoryCommittedBytes = getMetricValue(m_pdhInterface.get(), "memory_committed_bytes");
    pdhCache.memoryCommitLimit = getMetricValue(m_pdhInterface.get(), "memory_commit_limit");
    pdhCache.memoryFaultsPerSec = getMetricValue(m_pdhInterface.get(), "memory_page_faults_per_sec");
    pdhCache.memoryPagesPerSec = getMetricValue(m_pdhInterface.get(), "memory_pages_per_sec");
    pdhCache.memoryPoolNonPagedBytes = getMetricValue(m_pdhInterface.get(), "memory_pool_nonpaged_bytes");
    pdhCache.memoryPoolPagedBytes = getMetricValue(m_pdhInterface.get(), "memory_pool_paged_bytes");
    pdhCache.memorySystemCodeBytes = getMetricValue(m_pdhInterface.get(), "memory_system_code_bytes");
    pdhCache.memorySystemDriverBytes = getMetricValue(m_pdhInterface.get(), "memory_system_driver_bytes");
    
    // Disk I/O metrics
    pdhCache.ioReadRateMBs = getMetricValue(m_pdhInterface.get(), "disk_read_bytes_per_sec") / (1024.0 * 1024.0);
    pdhCache.ioWriteRateMBs = getMetricValue(m_pdhInterface.get(), "disk_write_bytes_per_sec") / (1024.0 * 1024.0);
    pdhCache.diskReadsPerSec = getMetricValue(m_pdhInterface.get(), "disk_reads_per_sec");
    pdhCache.diskWritesPerSec = getMetricValue(m_pdhInterface.get(), "disk_writes_per_sec");
    pdhCache.diskTransfersPerSec = getMetricValue(m_pdhInterface.get(), "disk_transfers_per_sec");
    pdhCache.diskBytesPerSec = getMetricValue(m_pdhInterface.get(), "disk_bytes_per_sec");
    pdhCache.diskAvgReadQueueLength = getMetricValue(m_pdhInterface.get(), "disk_avg_read_queue_length");
    pdhCache.diskAvgWriteQueueLength = getMetricValue(m_pdhInterface.get(), "disk_avg_write_queue_length");
    pdhCache.diskAvgQueueLength = getMetricValue(m_pdhInterface.get(), "disk_avg_queue_length");
    pdhCache.diskAvgReadTime = getMetricValue(m_pdhInterface.get(), "disk_avg_read_time");
    pdhCache.diskAvgWriteTime = getMetricValue(m_pdhInterface.get(), "disk_avg_write_time");
    pdhCache.diskAvgTransferTime = getMetricValue(m_pdhInterface.get(), "disk_avg_transfer_time");
    pdhCache.diskPercentTime = getMetricValue(m_pdhInterface.get(), "disk_percent_time");
    pdhCache.diskPercentReadTime = getMetricValue(m_pdhInterface.get(), "disk_percent_read_time");
    pdhCache.diskPercentWriteTime = getMetricValue(m_pdhInterface.get(), "disk_percent_write_time");
    
    // System kernel metrics
    pdhCache.contextSwitchesPerSec = getMetricValue(m_pdhInterface.get(), "system_context_switches_per_sec");
    pdhCache.systemProcessorQueueLength = getMetricValue(m_pdhInterface.get(), "system_processor_queue_length");
    pdhCache.systemProcesses = getMetricValue(m_pdhInterface.get(), "system_processes");
    pdhCache.systemThreads = getMetricValue(m_pdhInterface.get(), "system_threads");
    
    // Legacy compatibility metrics
    pdhCache.pdhInterruptsPerSec = getMetricValue(m_pdhInterface.get(), "system_system_calls_per_sec");
    
    // Per-core metrics collection (add to cache for consistency)
    bool perCoreUsageSuccess = m_pdhInterface->getPerCoreMetric("cpu_per_core_usage", pdhCache.perCoreCpuUsage);
    bool perCoreFreqSuccess = m_pdhInterface->getPerCoreMetric("cpu_per_core_actual_freq", pdhCache.perCoreActualFreq);
    
    // Debug logging for per-core cache collection (every 10 seconds)
    static auto lastCacheLog = std::chrono::steady_clock::now();
    auto cacheNow = std::chrono::steady_clock::now();
    if (std::chrono::duration_cast<std::chrono::seconds>(cacheNow - lastCacheLog).count() >= 10) {

      /*
      LogCritical("[PDH-CACHE] Per-core collection - Usage: " + std::to_string(perCoreUsageSuccess) + 
                  " (size: " + std::to_string(pdhCache.perCoreCpuUsage.size()) + 
                  "), Frequency: " + std::to_string(perCoreFreqSuccess) + 
                  " (size: " + std::to_string(pdhCache.perCoreActualFreq.size()) + ")"); */
      
      if (!pdhCache.perCoreCpuUsage.empty()) {
        std::string usageLog = "Usage values: [";
        for (size_t i = 0; i < std::min(pdhCache.perCoreCpuUsage.size(), size_t(8)); ++i) {
          if (i > 0) usageLog += ", ";
          usageLog += std::to_string(static_cast<int>(pdhCache.perCoreCpuUsage[i])) + "%";
        }
        if (pdhCache.perCoreCpuUsage.size() > 8) usageLog += "...";
        usageLog += "]";
        //LogCritical("[PDH-CACHE] " + usageLog);
      }
      
      if (!pdhCache.perCoreActualFreq.empty()) {
        std::string freqLog = "Frequency values: [";
        for (size_t i = 0; i < std::min(pdhCache.perCoreActualFreq.size(), size_t(8)); ++i) {
          if (i > 0) freqLog += ", ";
          freqLog += std::to_string(static_cast<int>(pdhCache.perCoreActualFreq[i])) + "MHz";
        }
        if (pdhCache.perCoreActualFreq.size() > 8) freqLog += "...";
        freqLog += "]";
        //LogCritical("[PDH-CACHE] " + freqLog);
      }
      
      lastCacheLog = cacheNow;
    }
    
    // If per-core collection failed, clear the vectors
    if (!perCoreUsageSuccess) {
      pdhCache.perCoreCpuUsage.clear();
      currentMissing++;
    }
    if (!perCoreFreqSuccess) {
      pdhCache.perCoreActualFreq.clear();
      currentMissing++;
    }
    
    // Calculate memory load using total memory from ConstantSystemInfo (constant, not collected every cycle)
    const auto& sysInfo = SystemMetrics::GetConstantSystemInfo();
    if (sysInfo.totalPhysicalMemoryMB > 0 && pdhCache.availableMemoryMB > 0) {
      double usedMemoryMB = sysInfo.totalPhysicalMemoryMB - pdhCache.availableMemoryMB;
      pdhCache.memoryLoad = (usedMemoryMB / sysInfo.totalPhysicalMemoryMB) * 100.0;
    } else {
      pdhCache.memoryLoad = -1.0; // Invalid data
    }
    
    // Update timestamp for PDH cache
    pdhCache.lastTimestamp = std::chrono::steady_clock::now();
    
    // Update missing count
    missingMetricsCount = currentMissing;

    // Fix the metrics success reporting to prevent negative numbers
    int successfulMetrics = std::max(0, totalMetricsExpected - currentMissing);
    
    // Periodic status reporting (every 10 seconds)
    auto now = std::chrono::steady_clock::now();
    if (BenchmarkLogger::shouldLogStatus()) {
      double successRate = totalMetricsExpected > 0 ? 
        ((double)successfulMetrics / totalMetricsExpected) * 100.0 : 0.0;
      
      BenchmarkLogger::logStatus(
        "PDH Metrics Status: " + std::to_string(successfulMetrics) + 
        "/" + std::to_string(totalMetricsExpected) + " available (" + 
        std::to_string((int)successRate) + "% success rate)");
      
      if (currentMissing > 0) {
        BenchmarkLogger::logStatus(
          "Missing " + std::to_string(currentMissing) + " metrics this cycle");
      }
    }

    // CSV-SNAPSHOT logging moved to main benchmark loop during RUNNING state only

    // Legacy CPU usage fields removed - all CPU data now comes from PDH perCoreCpuUsagePdh and procProcessorTime
  }
}




  void BenchmarkManager::performAutomaticUpload() {
    try {
      ApplicationSettings& settings = ApplicationSettings::getInstance();
    if (!settings.getEffectiveAutomaticDataUploadEnabled()) {
      LOG_INFO << "Benchmark upload disabled by settings (offline mode, data collection, or backend flags)";
      return;
    }

    // Only upload if the last run was valid and finalized
    if (!m_stateTracker || !m_stateTracker->isValidBenchmark() || !m_finalWriteDone) {
      LOG_INFO << "Skipping automatic upload: last run invalid or final write not completed";
      return;
    }

    LOG_INFO << "Starting automatic benchmark data upload...";

    // Upload only the CSV from the current run to avoid picking up stale files
    QString resultsPath = qApp->applicationDirPath() + "/benchmark_results";
    QDir dir(resultsPath);
    
    if (!dir.exists()) {
      LOG_WARN << "Benchmark results directory does not exist: " << resultsPath.toStdString();
      return;
    }

    QStringList filters;
    filters << "*.csv";
    QFileInfoList files = dir.entryInfoList(filters, QDir::Files, QDir::Time);
    
    if (files.isEmpty()) {
      LOG_WARN << "No CSV files found in benchmark results directory";
      return;
    }

    // Build expected filename for this run
    QString expectedCsv = QString("%1/%2").arg(resultsPath, m_outputFilename);
    QFileInfo expectedInfo(expectedCsv);
    QString csvPath;

    if (expectedInfo.exists() && expectedInfo.isFile()) {
      csvPath = expectedInfo.absoluteFilePath();
    } else {
      // Fallback: use the most recent file if expected not found (defensive)
      QFileInfo mostRecentFile = files.first();
      csvPath = mostRecentFile.absoluteFilePath();
      LOG_WARN << "Expected current run CSV not found, falling back to most recent: "
               << csvPath.toStdString();
    }
    
    LOG_INFO << "Found most recent benchmark file: " << csvPath.toStdString();
    
    // Look for associated files (specs, optimization settings)
    QStringList associatedFiles;
    associatedFiles << csvPath;
    
  QString baseName = QFileInfo(csvPath).baseName();
    
    // Look for specs file
    QStringList specsFilters;
    specsFilters << baseName + "_specs.txt" << baseName + "_specs.json";
    for (const QString& specsPattern : specsFilters) {
      QString specsPath = dir.absoluteFilePath(specsPattern);
      if (QFile::exists(specsPath)) {
        associatedFiles << specsPath;
        LOG_INFO << "Found specs file: " << specsPath.toStdString();
        break;
      }
    }
    
    // Look for optimization settings
    QString optPath = dir.absoluteFilePath("optimizationsettings.json");
    if (QFile::exists(optPath)) {
      associatedFiles << optPath;
      LOG_INFO << "Found optimization settings file: " << optPath.toStdString();
    }
    
    LOG_INFO << "Building upload request using PublicExportBuilder...";
    PublicExportBuilder builder;
    QVariant uploadReq = builder.buildUploadRequestVariant(csvPath, QString(), QString(), associatedFiles);
    
    if (!uploadReq.isValid()) {
      LOG_ERROR << "Failed to build upload request for automatic upload";
      return;
    }
    
    LOG_INFO << "Creating BenchmarkApiClient for automatic upload...";
    auto* api = new BenchmarkApiClient(this);
    
    LOG_INFO << "Starting automatic benchmark upload via BenchmarkApiClient...";
    api->uploadBenchmark(uploadReq, [this, api](bool success, const QString& err, QString runId){
      LOG_INFO << "Automatic upload callback received - success: " << success <<
               ", runId: " << runId.toStdString();
      
      // Clean up API client
      api->deleteLater();
      
      if (success) {
        LOG_INFO << "Automatic upload succeeded, runId=" << runId.toStdString();
      } else {
        LOG_ERROR << "Automatic upload failed: " << err.toStdString();
      }
    });
    
  } catch (const std::exception& e) {
    LogError("Exception during automatic upload: " + std::string(e.what()));
  } catch (...) {
    LogError("Unknown exception during automatic upload");
  }
}
